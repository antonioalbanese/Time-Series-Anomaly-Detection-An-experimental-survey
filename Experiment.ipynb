{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antonioalbanese/Time-Series-Anomaly-Detection-An-experimental-survey/blob/main/Experiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gxesA1FbjV5",
        "outputId": "e2c19f7e-df9a-439f-8f76-5c8d9ee0b029"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Time-Series-Anomaly-Detection-An-experimental-survey'...\n",
            "remote: Enumerating objects: 955, done.\u001b[K\n",
            "remote: Counting objects: 100% (438/438), done.\u001b[K\n",
            "remote: Compressing objects: 100% (183/183), done.\u001b[K\n",
            "remote: Total 955 (delta 298), reused 370 (delta 255), pack-reused 517\u001b[K\n",
            "Receiving objects: 100% (955/955), 2.78 MiB | 15.24 MiB/s, done.\n",
            "Resolving deltas: 100% (571/571), done.\n",
            "/content/Time-Series-Anomaly-Detection-An-experimental-survey\n"
          ]
        }
      ],
      "source": [
        "# !pip install wandb --quiet\n",
        "# !wandb login\n",
        "\n",
        "# import wandb\n",
        "\n",
        "!pip install torchinfo --quiet\n",
        "\n",
        "!git clone https://github.com/antonioalbanese/Time-Series-Anomaly-Detection-An-experimental-survey/\n",
        "\n",
        "%cd Time-Series-Anomaly-Detection-An-experimental-survey\n",
        "\n",
        "# %mkdir dataset\n",
        "# !gdown https://drive.google.com/uc?id=1ZCLBU_pKTbsPlcj_LwxZE3IRy6mrlys3 -O=\"./dataset/MSL.zip\"\n",
        "# !gdown https://drive.google.com/uc?id=18JNYBsaX7tu0Qfgo92nCBCklv471L1xB -O=\"./dataset/SMD.zip\"\n",
        "# import zipfile\n",
        "# with zipfile.ZipFile(\"./dataset/MSL.zip\", 'r') as zip_ref:\n",
        "#     zip_ref.extractall(\"./dataset/\")\n",
        "# with zipfile.ZipFile(\"./dataset/SMD.zip\", 'r') as zip_ref:\n",
        "#     zip_ref.extractall(\"./dataset/\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data\n",
        "!mkdir data/SWAT\n",
        "#normal period\n",
        "!python USAD/gdrivedl.py https://drive.google.com/open?id=1rVJ5ry5GG-ZZi5yI4x9lICB8VhErXwCw data/SWAT\n",
        "#anomalies\n",
        "!python USAD/gdrivedl.py https://drive.google.com/open?id=1iDYc0OEmidN712fquOBRFjln90SbpaE7 data/SWAT"
      ],
      "metadata": {
        "id": "bAm8xpOkP9Ph",
        "outputId": "3f3ac3d2-b770-4d34-f525-1bc806ad0406",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/SWAT/SWaT_Dataset_Normal_v1.csv\n",
            "[==================================================] 163.77MB/163.77MB\n",
            "data/SWAT/SWaT_Dataset_Attack_v0.csv\n",
            "[==================================================] 127.27MB/127.27MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### NEW USAD \n",
        "from AnomalyDetectionMethodClass import ADMethod\n",
        "\n",
        "configuration = {\n",
        "    'DATASET': 'SWAT', #SWAT, MSL, SMD, NAB\n",
        "    'DATAPATH': None, #\"realKnownCause/machine_temperature_system_failure.csv\", #Only needed with NAB\n",
        "    'SEQ_LEN': 100,\n",
        "    'STEP': 100,\n",
        "    'HIDDEN_SIZE': 100, #Needed for USAD method\n",
        "    'LR': 0.0001,\n",
        "    'EPOCHS': 50, \n",
        "    'VERBOSE': True\n",
        "}\n",
        "method = ADMethod(name = 'USAD', config = configuration)\n",
        "train_history = method.train()\n",
        "_, score = method.test()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yAzTRgG-5Ao1",
        "outputId": "d387c0e9-de52-4578-90a4-8198a917a2e5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=====================================================================\n",
            "Initializing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/Time-Series-Anomaly-Detection-An-experimental-survey/datafactory.py:25: DtypeWarning: Columns (26) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  self.load_data()\n",
            "/content/Time-Series-Anomaly-Detection-An-experimental-survey/datafactory.py:25: DtypeWarning: Columns (1,9,28,46) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  self.load_data()\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data preprocessing and method configuration finished in 28.59140944480896 sec.\n",
            "Model summary: \n",
            "=================================================================\n",
            "Layer (type:depth-idx)                   Param #\n",
            "=================================================================\n",
            "UsadModel                                --\n",
            "├─UsadEncoder: 1-1                       --\n",
            "│    └─Linear: 2-1                       13,007,550\n",
            "│    └─Linear: 2-2                       3,252,525\n",
            "│    └─Linear: 2-3                       12,760,000\n",
            "│    └─ReLU: 2-4                         --\n",
            "├─UsadDecoder: 1-2                       --\n",
            "│    └─Linear: 2-5                       12,751,275\n",
            "│    └─Linear: 2-6                       3,253,800\n",
            "│    └─Linear: 2-7                       13,010,100\n",
            "│    └─ReLU: 2-8                         --\n",
            "│    └─Sigmoid: 2-9                      --\n",
            "├─UsadDecoder: 1-3                       --\n",
            "│    └─Linear: 2-10                      12,751,275\n",
            "│    └─Linear: 2-11                      3,253,800\n",
            "│    └─Linear: 2-12                      13,010,100\n",
            "│    └─ReLU: 2-13                        --\n",
            "│    └─Sigmoid: 2-14                     --\n",
            "=================================================================\n",
            "Total params: 87,050,425\n",
            "Trainable params: 87,050,425\n",
            "Non-trainable params: 0\n",
            "=================================================================\n",
            "=====================================================================\n",
            "Training...\n",
            "Epoch 1/200: train_loss_1:0.05178328966721892. train_loss_2:0.07133058253675699\n",
            "Epoch 2/200: train_loss_1:0.04474093858152628. train_loss_2:-0.002087238850072026\n",
            "Epoch 3/200: train_loss_1:0.04875724967569113. train_loss_2:-0.021469175815582275\n",
            "Epoch 4/200: train_loss_1:0.05272333156317473. train_loss_2:-0.034913004748523234\n",
            "Epoch 5/200: train_loss_1:0.05542444307357073. train_loss_2:-0.0423261608928442\n",
            "Epoch 6/200: train_loss_1:0.05664700753986836. train_loss_2:-0.04662807248532772\n",
            "Epoch 7/200: train_loss_1:0.057751042768359184. train_loss_2:-0.04964799489825964\n",
            "Epoch 8/200: train_loss_1:0.05820783488452434. train_loss_2:-0.051515357196331026\n",
            "Epoch 9/200: train_loss_1:0.05941665396094322. train_loss_2:-0.05324961561709642\n",
            "Epoch 10/200: train_loss_1:0.059999079629778865. train_loss_2:-0.05434402674436569\n",
            "Epoch 11/200: train_loss_1:0.06627421043813228. train_loss_2:-0.05741691654548049\n",
            "Epoch 12/200: train_loss_1:0.07905265614390374. train_loss_2:-0.06924648396670818\n",
            "Epoch 13/200: train_loss_1:0.07918004654347896. train_loss_2:-0.07044757474213839\n",
            "Epoch 14/200: train_loss_1:0.07917894050478935. train_loss_2:-0.07203393634408713\n",
            "Epoch 15/200: train_loss_1:0.0798764418810606. train_loss_2:-0.07432157564908266\n",
            "Epoch 16/200: train_loss_1:0.08065486960113048. train_loss_2:-0.07284844182431698\n",
            "Epoch 17/200: train_loss_1:0.08164506554603576. train_loss_2:-0.07352633960545063\n",
            "Epoch 18/200: train_loss_1:0.08154882974922657. train_loss_2:-0.0740709776058793\n",
            "Epoch 19/200: train_loss_1:0.08089323937892914. train_loss_2:-0.0745582889765501\n",
            "Epoch 20/200: train_loss_1:0.08101440630853177. train_loss_2:-0.07499686274677515\n",
            "Epoch 21/200: train_loss_1:0.08101835362613201. train_loss_2:-0.07539367526769639\n",
            "Epoch 22/200: train_loss_1:0.08110316917300224. train_loss_2:-0.0757544081658125\n",
            "Epoch 23/200: train_loss_1:0.08117656894028187. train_loss_2:-0.07608377765864134\n",
            "Epoch 24/200: train_loss_1:0.08124669082462788. train_loss_2:-0.07638569958508015\n",
            "Epoch 25/200: train_loss_1:0.08132020756602287. train_loss_2:-0.07666349112987518\n",
            "Epoch 26/200: train_loss_1:0.08129639066755771. train_loss_2:-0.07692009471356868\n",
            "Epoch 27/200: train_loss_1:0.08115383759140968. train_loss_2:-0.07716012746095657\n",
            "Epoch 28/200: train_loss_1:0.08115514479577542. train_loss_2:-0.07738953046500682\n",
            "Epoch 29/200: train_loss_1:0.08110923990607262. train_loss_2:-0.07759591154754161\n",
            "Epoch 30/200: train_loss_1:0.08116676434874534. train_loss_2:-0.07779124937951565\n",
            "Epoch 31/200: train_loss_1:0.08122677505016326. train_loss_2:-0.07797287330031395\n",
            "Epoch 32/200: train_loss_1:0.0811030127108097. train_loss_2:-0.07814403511583805\n",
            "Epoch 33/200: train_loss_1:0.08135008476674557. train_loss_2:-0.0783002559095621\n",
            "Epoch 34/200: train_loss_1:0.08139830306172371. train_loss_2:-0.07844896800816059\n",
            "Epoch 35/200: train_loss_1:0.08144002258777619. train_loss_2:-0.07858879566192627\n",
            "Epoch 36/200: train_loss_1:0.08149676695466042. train_loss_2:-0.07872066721320152\n",
            "Epoch 37/200: train_loss_1:0.08153466694056988. train_loss_2:-0.07884537018835544\n",
            "Epoch 38/200: train_loss_1:0.08156597539782524. train_loss_2:-0.07896559126675129\n",
            "Epoch 39/200: train_loss_1:0.10669910721480846. train_loss_2:-0.08494778834283352\n",
            "Epoch 40/200: train_loss_1:0.0833393607288599. train_loss_2:-0.07916352190077305\n",
            "Epoch 41/200: train_loss_1:0.08337882831692696. train_loss_2:-0.07926501035690307\n",
            "Epoch 42/200: train_loss_1:0.08335163742303849. train_loss_2:-0.07936180047690869\n",
            "Epoch 43/200: train_loss_1:0.08335622698068619. train_loss_2:-0.07945413514971733\n",
            "Epoch 44/200: train_loss_1:0.08340614549815654. train_loss_2:-0.07955213524401188\n",
            "Epoch 45/200: train_loss_1:0.0834490817040205. train_loss_2:-0.07969012074172496\n",
            "Epoch 46/200: train_loss_1:0.0834733072668314. train_loss_2:-0.07977069690823554\n",
            "Epoch 47/200: train_loss_1:0.0834758911281824. train_loss_2:-0.07984784655272961\n",
            "Epoch 48/200: train_loss_1:0.08345971293747426. train_loss_2:-0.07992178052663804\n",
            "Epoch 49/200: train_loss_1:0.08346501812338829. train_loss_2:-0.07999269552528858\n",
            "Epoch 50/200: train_loss_1:0.08346353843808174. train_loss_2:-0.080060775578022\n",
            "Epoch 51/200: train_loss_1:0.0834727045148611. train_loss_2:-0.08016844280064106\n",
            "Epoch 52/200: train_loss_1:0.08349835388362407. train_loss_2:-0.0802533183246851\n",
            "Epoch 53/200: train_loss_1:0.08349462822079659. train_loss_2:-0.08031388893723487\n",
            "Epoch 54/200: train_loss_1:0.08349131830036641. train_loss_2:-0.08037221282720566\n",
            "Epoch 55/200: train_loss_1:0.08348983414471149. train_loss_2:-0.08042841851711273\n",
            "Epoch 56/200: train_loss_1:0.08348512873053551. train_loss_2:-0.08048261553049088\n",
            "Epoch 57/200: train_loss_1:0.08347773551940918. train_loss_2:-0.08053491115570069\n",
            "Epoch 58/200: train_loss_1:0.08349842801690102. train_loss_2:-0.08058540374040604\n",
            "Epoch 59/200: train_loss_1:0.0834723450243473. train_loss_2:-0.08063418008387088\n",
            "Epoch 60/200: train_loss_1:0.0834722526371479. train_loss_2:-0.08068133741617203\n",
            "Epoch 61/200: train_loss_1:0.08347097337245941. train_loss_2:-0.08072694763541222\n",
            "Epoch 62/200: train_loss_1:0.08347234800457955. train_loss_2:-0.08077108487486839\n",
            "Epoch 63/200: train_loss_1:0.08347170427441597. train_loss_2:-0.08081381767988205\n",
            "Epoch 64/200: train_loss_1:0.08347067050635815. train_loss_2:-0.0808543000370264\n",
            "Epoch 65/200: train_loss_1:0.08347256220877171. train_loss_2:-0.08089534640312195\n",
            "Epoch 66/200: train_loss_1:0.08347127325832844. train_loss_2:-0.08093425780534744\n",
            "Epoch 67/200: train_loss_1:0.08347105644643307. train_loss_2:-0.08097200654447079\n",
            "Epoch 68/200: train_loss_1:0.08347005657851696. train_loss_2:-0.08100864291191101\n",
            "Epoch 69/200: train_loss_1:0.08346937038004398. train_loss_2:-0.08104422204196453\n",
            "Epoch 70/200: train_loss_1:0.08346808962523937. train_loss_2:-0.08107877857983112\n",
            "Epoch 71/200: train_loss_1:0.0834702055901289. train_loss_2:-0.08111236803233624\n",
            "Epoch 72/200: train_loss_1:0.08346360921859741. train_loss_2:-0.08114502243697644\n",
            "Epoch 73/200: train_loss_1:0.08346335515379906. train_loss_2:-0.08117678239941598\n",
            "Epoch 74/200: train_loss_1:0.08346285931766033. train_loss_2:-0.08120768405497074\n",
            "Epoch 75/200: train_loss_1:0.08346254266798496. train_loss_2:-0.08123776093125343\n",
            "Epoch 76/200: train_loss_1:0.08346264213323593. train_loss_2:-0.08126704245805741\n",
            "Epoch 77/200: train_loss_1:0.08346220478415489. train_loss_2:-0.08129557222127914\n",
            "Epoch 78/200: train_loss_1:0.08346215710043907. train_loss_2:-0.08132335580885411\n",
            "Epoch 79/200: train_loss_1:0.08346119783818721. train_loss_2:-0.08135045394301414\n",
            "Epoch 80/200: train_loss_1:0.08346201926469803. train_loss_2:-0.0813768371939659\n",
            "Epoch 81/200: train_loss_1:0.08346198350191117. train_loss_2:-0.08140262812376023\n",
            "Epoch 82/200: train_loss_1:0.08346181586384774. train_loss_2:-0.0814276684075594\n",
            "Epoch 83/200: train_loss_1:0.0834618430584669. train_loss_2:-0.08145228587090969\n",
            "Epoch 84/200: train_loss_1:0.08346176333725452. train_loss_2:-0.08147616907954217\n",
            "Epoch 85/200: train_loss_1:0.08346174657344818. train_loss_2:-0.08149958178400993\n",
            "Epoch 86/200: train_loss_1:0.08346392773091793. train_loss_2:-0.08152244128286838\n",
            "Epoch 87/200: train_loss_1:0.08346220143139363. train_loss_2:-0.0815447498112917\n",
            "Epoch 88/200: train_loss_1:0.0834622897207737. train_loss_2:-0.08156655505299568\n",
            "Epoch 89/200: train_loss_1:0.08346210904419422. train_loss_2:-0.0815878689289093\n",
            "Epoch 90/200: train_loss_1:0.08346199579536914. train_loss_2:-0.0816087055951357\n",
            "Epoch 91/200: train_loss_1:0.08346197009086609. train_loss_2:-0.08162908777594566\n",
            "Epoch 92/200: train_loss_1:0.08346207737922669. train_loss_2:-0.08164903074502945\n",
            "Epoch 93/200: train_loss_1:0.08346187211573124. train_loss_2:-0.08166853561997414\n",
            "Epoch 94/200: train_loss_1:0.0834618054330349. train_loss_2:-0.08168762996792793\n",
            "Epoch 95/200: train_loss_1:0.08346182145178319. train_loss_2:-0.08170632310211659\n",
            "Epoch 96/200: train_loss_1:0.08346188440918922. train_loss_2:-0.0817246250808239\n",
            "Epoch 97/200: train_loss_1:0.08346186652779579. train_loss_2:-0.08174255564808845\n",
            "Epoch 98/200: train_loss_1:0.0834618128836155. train_loss_2:-0.0817601129412651\n",
            "Epoch 99/200: train_loss_1:0.08346130102872848. train_loss_2:-0.08177731856703759\n",
            "Epoch 100/200: train_loss_1:0.08346147872507573. train_loss_2:-0.08179418109357357\n",
            "Epoch 101/200: train_loss_1:0.08346148617565632. train_loss_2:-0.08181071020662785\n",
            "Epoch 102/200: train_loss_1:0.0834614910185337. train_loss_2:-0.08182691037654877\n",
            "Epoch 103/200: train_loss_1:0.08346148431301117. train_loss_2:-0.08184279911220074\n",
            "Epoch 104/200: train_loss_1:0.08346147313714028. train_loss_2:-0.08185838460922241\n",
            "Epoch 105/200: train_loss_1:0.08346148431301117. train_loss_2:-0.08187366910278797\n",
            "Epoch 106/200: train_loss_1:0.08346149623394013. train_loss_2:-0.08188866525888443\n",
            "Epoch 107/200: train_loss_1:0.08346151709556579. train_loss_2:-0.08190338686108589\n",
            "Epoch 108/200: train_loss_1:0.08346152603626251. train_loss_2:-0.08191782906651497\n",
            "Epoch 109/200: train_loss_1:0.08346154652535916. train_loss_2:-0.08193201273679733\n",
            "Epoch 110/200: train_loss_1:0.08346156142652035. train_loss_2:-0.08194593340158463\n",
            "Epoch 111/200: train_loss_1:0.0834615770727396. train_loss_2:-0.08195960409939289\n",
            "Epoch 112/200: train_loss_1:0.08346159048378468. train_loss_2:-0.08197302892804145\n",
            "Epoch 113/200: train_loss_1:0.08346160687506199. train_loss_2:-0.08198622092604638\n",
            "Epoch 114/200: train_loss_1:0.083461619541049. train_loss_2:-0.08199917785823345\n",
            "Epoch 115/200: train_loss_1:0.08346163518726826. train_loss_2:-0.08201191127300263\n",
            "Epoch 116/200: train_loss_1:0.08346165120601653. train_loss_2:-0.08202442564070225\n",
            "Epoch 117/200: train_loss_1:0.08346166610717773. train_loss_2:-0.0820367269217968\n",
            "Epoch 118/200: train_loss_1:0.08346167765557766. train_loss_2:-0.08204881586134434\n",
            "Epoch 119/200: train_loss_1:0.08346168883144855. train_loss_2:-0.08206070140004158\n",
            "Epoch 120/200: train_loss_1:0.08346166349947452. train_loss_2:-0.08207239471375942\n",
            "Epoch 121/200: train_loss_1:0.08346181139349937. train_loss_2:-0.08208388835191727\n",
            "Epoch 122/200: train_loss_1:0.0834618266671896. train_loss_2:-0.08209520019590855\n",
            "Epoch 123/200: train_loss_1:0.08346183747053146. train_loss_2:-0.08210632242262364\n",
            "Epoch 124/200: train_loss_1:0.08346185013651848. train_loss_2:-0.08211726620793343\n",
            "Epoch 125/200: train_loss_1:0.08346186131238938. train_loss_2:-0.08212803527712823\n",
            "Epoch 126/200: train_loss_1:0.0834618654102087. train_loss_2:-0.08213863298296928\n",
            "Epoch 127/200: train_loss_1:0.08346186131238938. train_loss_2:-0.08214906603097916\n",
            "Epoch 128/200: train_loss_1:0.08346177861094475. train_loss_2:-0.08215933591127396\n",
            "Epoch 129/200: train_loss_1:0.08346287459135056. train_loss_2:-0.08216944187879563\n",
            "Epoch 130/200: train_loss_1:0.08346422612667084. train_loss_2:-0.08217939622700214\n",
            "Epoch 131/200: train_loss_1:0.08346459120512009. train_loss_2:-0.08218920156359673\n",
            "Epoch 132/200: train_loss_1:0.0834656834602356. train_loss_2:-0.0821988545358181\n",
            "Epoch 133/200: train_loss_1:0.08346563130617142. train_loss_2:-0.08220836520195007\n",
            "Epoch 134/200: train_loss_1:0.08346560932695865. train_loss_2:-0.08221772611141205\n",
            "Epoch 135/200: train_loss_1:0.08346559181809425. train_loss_2:-0.08222695402801036\n",
            "Epoch 136/200: train_loss_1:0.08346557542681694. train_loss_2:-0.08223604559898376\n",
            "Epoch 137/200: train_loss_1:0.08346556350588799. train_loss_2:-0.08224500939249993\n",
            "Epoch 138/200: train_loss_1:0.0834655448794365. train_loss_2:-0.08225383497774601\n",
            "Epoch 139/200: train_loss_1:0.08346553109586238. train_loss_2:-0.08226254023611546\n",
            "Epoch 140/200: train_loss_1:0.08346551619470119. train_loss_2:-0.0822711180895567\n",
            "Epoch 141/200: train_loss_1:0.08346550092101097. train_loss_2:-0.08227957375347614\n",
            "Epoch 142/200: train_loss_1:0.08346548676490784. train_loss_2:-0.08228791169822217\n",
            "Epoch 143/200: train_loss_1:0.08346546851098538. train_loss_2:-0.08229613117873669\n",
            "Epoch 144/200: train_loss_1:0.08346545733511448. train_loss_2:-0.08230423852801323\n",
            "Epoch 145/200: train_loss_1:0.08346543908119201. train_loss_2:-0.08231223076581955\n",
            "Epoch 146/200: train_loss_1:0.08346542492508888. train_loss_2:-0.08232011757791043\n",
            "Epoch 147/200: train_loss_1:0.08346540816128253. train_loss_2:-0.08232789300382137\n",
            "Epoch 148/200: train_loss_1:0.08346539251506328. train_loss_2:-0.082335564494133\n",
            "Epoch 149/200: train_loss_1:0.08346537984907627. train_loss_2:-0.08234313428401947\n",
            "Epoch 150/200: train_loss_1:0.08346536867320538. train_loss_2:-0.08235060423612595\n",
            "Epoch 151/200: train_loss_1:0.08346535861492158. train_loss_2:-0.08235797733068466\n",
            "Epoch 152/200: train_loss_1:0.08346534818410874. train_loss_2:-0.08236525058746338\n",
            "Epoch 153/200: train_loss_1:0.08346533551812171. train_loss_2:-0.08237242624163628\n",
            "Epoch 154/200: train_loss_1:0.08346532173454761. train_loss_2:-0.08237951025366783\n",
            "Epoch 155/200: train_loss_1:0.0834653090685606. train_loss_2:-0.08238650374114513\n",
            "Epoch 156/200: train_loss_1:0.08346529603004456. train_loss_2:-0.08239340782165527\n",
            "Epoch 157/200: train_loss_1:0.08346528038382531. train_loss_2:-0.08240022286772727\n",
            "Epoch 158/200: train_loss_1:0.08346526660025119. train_loss_2:-0.08240695185959339\n",
            "Epoch 159/200: train_loss_1:0.08346525430679322. train_loss_2:-0.08241359628736973\n",
            "Epoch 160/200: train_loss_1:0.0834652416408062. train_loss_2:-0.08242015540599823\n",
            "Epoch 161/200: train_loss_1:0.08346523158252239. train_loss_2:-0.0824266392737627\n",
            "Epoch 162/200: train_loss_1:0.0834652192890644. train_loss_2:-0.0824330385774374\n",
            "Epoch 163/200: train_loss_1:0.08346520811319351. train_loss_2:-0.0824393603950739\n",
            "Epoch 164/200: train_loss_1:0.08346519693732261. train_loss_2:-0.08244560435414314\n",
            "Epoch 165/200: train_loss_1:0.08346518985927105. train_loss_2:-0.08245177567005157\n",
            "Epoch 166/200: train_loss_1:0.08346517719328403. train_loss_2:-0.0824578657746315\n",
            "Epoch 167/200: train_loss_1:0.08346517309546471. train_loss_2:-0.08246389143168927\n",
            "Epoch 168/200: train_loss_1:0.0834651630371809. train_loss_2:-0.08246984072029591\n",
            "Epoch 169/200: train_loss_1:0.08346515223383903. train_loss_2:-0.082475720718503\n",
            "Epoch 170/200: train_loss_1:0.08346513770520687. train_loss_2:-0.08248152695596218\n",
            "Epoch 171/200: train_loss_1:0.08346512839198113. train_loss_2:-0.08248727284371853\n",
            "Epoch 172/200: train_loss_1:0.08346511423587799. train_loss_2:-0.08249294646084308\n",
            "Epoch 173/200: train_loss_1:0.0834651030600071. train_loss_2:-0.08249855637550355\n",
            "Epoch 174/200: train_loss_1:0.0834650918841362. train_loss_2:-0.08250409997999668\n",
            "Epoch 175/200: train_loss_1:0.08346508219838142. train_loss_2:-0.08250958248972892\n",
            "Epoch 176/200: train_loss_1:0.08346507400274276. train_loss_2:-0.08251500278711318\n",
            "Epoch 177/200: train_loss_1:0.08346506394445896. train_loss_2:-0.08252035938203335\n",
            "Epoch 178/200: train_loss_1:0.08346505835652351. train_loss_2:-0.08252565972507\n",
            "Epoch 179/200: train_loss_1:0.08346505388617516. train_loss_2:-0.08253090009093285\n",
            "Epoch 180/200: train_loss_1:0.08346504010260106. train_loss_2:-0.08253607787191868\n",
            "Epoch 181/200: train_loss_1:0.08346503004431724. train_loss_2:-0.08254120163619519\n",
            "Epoch 182/200: train_loss_1:0.08346501775085927. train_loss_2:-0.0825462680310011\n",
            "Epoch 183/200: train_loss_1:0.08346500732004643. train_loss_2:-0.08255128152668476\n",
            "Epoch 184/200: train_loss_1:0.08346499688923359. train_loss_2:-0.08255623802542686\n",
            "Epoch 185/200: train_loss_1:0.08346498683094979. train_loss_2:-0.08256114162504673\n",
            "Epoch 186/200: train_loss_1:0.08346498385071754. train_loss_2:-0.0825659953057766\n",
            "Epoch 187/200: train_loss_1:0.08346497491002083. train_loss_2:-0.08257079198956489\n",
            "Epoch 188/200: train_loss_1:0.08346497602760791. train_loss_2:-0.08257554136216641\n",
            "Epoch 189/200: train_loss_1:0.08346496932208539. train_loss_2:-0.08258023709058762\n",
            "Epoch 190/200: train_loss_1:0.08346495665609836. train_loss_2:-0.08258489072322846\n",
            "Epoch 191/200: train_loss_1:0.08346493765711785. train_loss_2:-0.08258948661386967\n",
            "Epoch 192/200: train_loss_1:0.08346493095159531. train_loss_2:-0.08259404227137565\n",
            "Epoch 193/200: train_loss_1:0.08346491716802121. train_loss_2:-0.08259854242205619\n",
            "Epoch 194/200: train_loss_1:0.08346492201089858. train_loss_2:-0.08260300643742084\n",
            "Epoch 195/200: train_loss_1:0.08346492014825344. train_loss_2:-0.08260741233825683\n",
            "Epoch 196/200: train_loss_1:0.08346491307020187. train_loss_2:-0.0826117854565382\n",
            "Epoch 197/200: train_loss_1:0.08346489630639553. train_loss_2:-0.08261609934270382\n",
            "Epoch 198/200: train_loss_1:0.08346488550305367. train_loss_2:-0.0826203890144825\n",
            "Epoch 199/200: train_loss_1:0.08346486799418926. train_loss_2:-0.0826246190816164\n",
            "Epoch 200/200: train_loss_1:0.08346486277878284. train_loss_2:-0.08262881748378277\n",
            "Training finished in 612.7059233188629 sec., avg time per epoch: 3.0635296165943147 sec.\n",
            "=====================================================================\n",
            "=====================================================================\n",
            "Testing...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-7584f39daa38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mADMethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'USAD'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfiguration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtrain_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/Time-Series-Anomaly-Detection-An-experimental-survey/AnomalyDetectionMethodClass.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    164\u001b[0m                                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m                                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestUsad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Time-Series-Anomaly-Detection-An-experimental-survey/AnomalyDetectionMethodClass.py\u001b[0m in \u001b[0;36mtestUsad\u001b[0;34m(model, loader, device)\u001b[0m\n\u001b[1;32m    344\u001b[0m                         \u001b[0mw1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m                         \u001b[0mw2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m                         \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m \t\tscores = np.concatenate([torch.stack(r[:-1]).flatten().detach().cpu().numpy(),\n",
            "\u001b[0;31mNameError\u001b[0m: name 'alpha' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = method.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOUmpudU-nvj",
        "outputId": "74fbb78d-92a3-4b65-df82-508dd348dbc3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=====================================================================\n",
            "Training...\n",
            "Epoch 1/100: train_loss_1:0.02646759055758953. train_loss_2:0.02633265929977858\n",
            "Epoch 2/100: train_loss_1:0.029782477365289297. train_loss_2:-0.009821017576025132\n",
            "Epoch 3/100: train_loss_1:0.022938725529928452. train_loss_2:-0.00954825119374113\n",
            "Epoch 4/100: train_loss_1:0.03228981701231757. train_loss_2:-0.021869422078138377\n",
            "Epoch 5/100: train_loss_1:0.02025915036426374. train_loss_2:-0.013489625632998613\n",
            "Epoch 6/100: train_loss_1:0.02796821907179913. train_loss_2:-0.0209273258626553\n",
            "Epoch 7/100: train_loss_1:0.041880360902229574. train_loss_2:-0.03539721559471577\n",
            "Epoch 8/100: train_loss_1:0.044326835500225883. train_loss_2:-0.0395004357136556\n",
            "Epoch 9/100: train_loss_1:0.03220559053991082. train_loss_2:-0.026503733242861927\n",
            "Epoch 10/100: train_loss_1:0.0683128937251038. train_loss_2:-0.0584050193199037\n",
            "Epoch 11/100: train_loss_1:0.06654218888991041. train_loss_2:-0.05834852048644313\n",
            "Epoch 12/100: train_loss_1:0.06465578692056882. train_loss_2:-0.0596181133095129\n",
            "Epoch 13/100: train_loss_1:0.06880279454506105. train_loss_2:-0.0643021817330593\n",
            "Epoch 14/100: train_loss_1:0.06833802384358866. train_loss_2:-0.06444100245695424\n",
            "Epoch 15/100: train_loss_1:0.0692184546985376. train_loss_2:-0.06554342981105969\n",
            "Epoch 16/100: train_loss_1:0.06947196615330967. train_loss_2:-0.0660170931024132\n",
            "Epoch 17/100: train_loss_1:0.07456692538143676. train_loss_2:-0.0698895522703727\n",
            "Epoch 18/100: train_loss_1:0.07326606674878686. train_loss_2:-0.0697913176069657\n",
            "Epoch 19/100: train_loss_1:0.14997186472662438. train_loss_2:-0.1392635602136085\n",
            "Epoch 20/100: train_loss_1:0.1711121195996249. train_loss_2:-0.16090061561560925\n",
            "Epoch 21/100: train_loss_1:0.17160726881321567. train_loss_2:-0.1616377713687626\n",
            "Epoch 22/100: train_loss_1:0.17157737827963299. train_loss_2:-0.16208452124286582\n",
            "Epoch 23/100: train_loss_1:0.1715982300631794. train_loss_2:-0.1631774166483938\n",
            "Epoch 24/100: train_loss_1:0.17155351655350792. train_loss_2:-0.16314978474452171\n",
            "Epoch 25/100: train_loss_1:0.17291347811251512. train_loss_2:-0.16374859232225536\n",
            "Epoch 26/100: train_loss_1:0.17226418024963802. train_loss_2:-0.16426410286882778\n",
            "Epoch 27/100: train_loss_1:0.17227793448133233. train_loss_2:-0.15948365564331596\n",
            "Epoch 28/100: train_loss_1:0.17229308629477466. train_loss_2:-0.15984660819356825\n",
            "Epoch 29/100: train_loss_1:0.1717826697377511. train_loss_2:-0.16027059847557987\n",
            "Epoch 30/100: train_loss_1:0.17042811886395937. train_loss_2:-0.16066632640582543\n",
            "Epoch 31/100: train_loss_1:0.17011392125744879. train_loss_2:-0.16103652671531396\n",
            "Epoch 32/100: train_loss_1:0.16825479600164625. train_loss_2:-0.16138359333997893\n",
            "Epoch 33/100: train_loss_1:0.1675592745527809. train_loss_2:-0.16170962541191666\n",
            "Epoch 34/100: train_loss_1:0.16766265761337162. train_loss_2:-0.1620164645122893\n",
            "Epoch 35/100: train_loss_1:0.1677646017000999. train_loss_2:-0.1623057845382043\n",
            "Epoch 36/100: train_loss_1:0.16786265879133602. train_loss_2:-0.16257902492343643\n",
            "Epoch 37/100: train_loss_1:0.16796943563737987. train_loss_2:-0.16283750037352243\n",
            "Epoch 38/100: train_loss_1:0.16807314771928905. train_loss_2:-0.16308236352087538\n",
            "Epoch 39/100: train_loss_1:0.16817328168286216. train_loss_2:-0.1633146787867134\n",
            "Epoch 40/100: train_loss_1:0.16826997596172638. train_loss_2:-0.163535377861541\n",
            "Epoch 41/100: train_loss_1:0.16836352499178897. train_loss_2:-0.16374530772000184\n",
            "Epoch 42/100: train_loss_1:0.16844881040814483. train_loss_2:-0.1639452322765633\n",
            "Epoch 43/100: train_loss_1:0.1685333654836372. train_loss_2:-0.16413586798273486\n",
            "Epoch 44/100: train_loss_1:0.1686144706643658. train_loss_2:-0.16431784197504137\n",
            "Epoch 45/100: train_loss_1:0.1686910371537562. train_loss_2:-0.16449172104950305\n",
            "Epoch 46/100: train_loss_1:0.16876615013605284. train_loss_2:-0.16465804183188779\n",
            "Epoch 47/100: train_loss_1:0.16923834374289454. train_loss_2:-0.16481728658631997\n",
            "Epoch 48/100: train_loss_1:0.16935935469321262. train_loss_2:-0.1649698982084239\n",
            "Epoch 49/100: train_loss_1:0.16941011163555544. train_loss_2:-0.16511627590214764\n",
            "Epoch 50/100: train_loss_1:0.16946473138199913. train_loss_2:-0.16525680121080374\n",
            "Epoch 51/100: train_loss_1:0.1695170592930582. train_loss_2:-0.16539181244594078\n",
            "Epoch 52/100: train_loss_1:0.1693413319227136. train_loss_2:-0.16552163136226158\n",
            "Epoch 53/100: train_loss_1:0.16922566175092885. train_loss_2:-0.1656465569028148\n",
            "Epoch 54/100: train_loss_1:0.1692653770248095. train_loss_2:-0.16576685278136052\n",
            "Epoch 55/100: train_loss_1:0.16931880826567425. train_loss_2:-0.1658827755370258\n",
            "Epoch 56/100: train_loss_1:0.16936804832499705. train_loss_2:-0.1659945610128803\n",
            "Epoch 57/100: train_loss_1:0.16941275935114167. train_loss_2:-0.16610241414588175\n",
            "Epoch 58/100: train_loss_1:0.16945869319232892. train_loss_2:-0.16620656820359053\n",
            "Epoch 59/100: train_loss_1:0.16950226418765973. train_loss_2:-0.16630740574112646\n",
            "Epoch 60/100: train_loss_1:0.16954600994969593. train_loss_2:-0.1664044177274645\n",
            "Epoch 61/100: train_loss_1:0.1695877531612361. train_loss_2:-0.1664984859066245\n",
            "Epoch 62/100: train_loss_1:0.16962777344900884. train_loss_2:-0.16658951783621753\n",
            "Epoch 63/100: train_loss_1:0.16966752965509155. train_loss_2:-0.1666776579287317\n",
            "Epoch 64/100: train_loss_1:0.1697064479008133. train_loss_2:-0.16676304553761895\n",
            "Epoch 65/100: train_loss_1:0.16974447364056552. train_loss_2:-0.1668458079666267\n",
            "Epoch 66/100: train_loss_1:0.1697805215731079. train_loss_2:-0.16692606221746514\n",
            "Epoch 67/100: train_loss_1:0.16982143252720067. train_loss_2:-0.16701040794084102\n",
            "Epoch 68/100: train_loss_1:0.169848080770469. train_loss_2:-0.16707949616290904\n",
            "Epoch 69/100: train_loss_1:0.1745023308896724. train_loss_2:-0.1717002346743772\n",
            "Epoch 70/100: train_loss_1:0.17755281382504803. train_loss_2:-0.1747527449955175\n",
            "Epoch 71/100: train_loss_1:0.1758286480182483. train_loss_2:-0.17307256502501758\n",
            "Epoch 72/100: train_loss_1:0.14000150354372132. train_loss_2:-0.13757896547516188\n",
            "Epoch 73/100: train_loss_1:0.10678686196973294. train_loss_2:-0.10496915853869768\n",
            "Epoch 74/100: train_loss_1:0.10597718686417297. train_loss_2:-0.10426428693312186\n",
            "Epoch 75/100: train_loss_1:0.10598517430049402. train_loss_2:-0.10430519664545118\n",
            "Epoch 76/100: train_loss_1:0.1059701076168337. train_loss_2:-0.10434378614580189\n",
            "Epoch 77/100: train_loss_1:0.11053737041390972. train_loss_2:-0.10860542391921267\n",
            "Epoch 78/100: train_loss_1:0.11266134260797206. train_loss_2:-0.11065985207204465\n",
            "Epoch 79/100: train_loss_1:0.11265500823472753. train_loss_2:-0.1106967157051887\n",
            "Epoch 80/100: train_loss_1:0.11264838301289229. train_loss_2:-0.1107326595135677\n",
            "Epoch 81/100: train_loss_1:0.11264416644418682. train_loss_2:-0.11076771191976688\n",
            "Epoch 82/100: train_loss_1:0.1126513138965324. train_loss_2:-0.11080191063660162\n",
            "Epoch 83/100: train_loss_1:0.11266230741216812. train_loss_2:-0.11083528385670097\n",
            "Epoch 84/100: train_loss_1:0.11267323957549201. train_loss_2:-0.11086786165833473\n",
            "Epoch 85/100: train_loss_1:0.1139736791819702. train_loss_2:-0.112174127480866\n",
            "Epoch 86/100: train_loss_1:0.11397743459652972. train_loss_2:-0.11219884409212771\n",
            "Epoch 87/100: train_loss_1:0.11398773152887086. train_loss_2:-0.11222954729089031\n",
            "Epoch 88/100: train_loss_1:0.11399779712528359. train_loss_2:-0.11225955873543833\n",
            "Epoch 89/100: train_loss_1:0.11400762770646884. train_loss_2:-0.11228889171723966\n",
            "Epoch 90/100: train_loss_1:0.11401723265463923. train_loss_2:-0.11231756992178199\n",
            "Epoch 91/100: train_loss_1:0.11402662889457044. train_loss_2:-0.11234562443914237\n",
            "Epoch 92/100: train_loss_1:0.1140358138507531. train_loss_2:-0.11237306658316541\n",
            "Epoch 93/100: train_loss_1:0.1140447968594086. train_loss_2:-0.11239991902753159\n",
            "Epoch 94/100: train_loss_1:0.11405358178380096. train_loss_2:-0.11242619786917428\n",
            "Epoch 95/100: train_loss_1:0.11406218016773094. train_loss_2:-0.11245192591974765\n",
            "Epoch 96/100: train_loss_1:0.11407058938969801. train_loss_2:-0.11247711260745555\n",
            "Epoch 97/100: train_loss_1:0.11407882752425877. train_loss_2:-0.11250178566501465\n",
            "Epoch 98/100: train_loss_1:0.11408689314568485. train_loss_2:-0.11252595548644478\n",
            "Epoch 99/100: train_loss_1:0.11409478919741547. train_loss_2:-0.11254963352356429\n",
            "Epoch 100/100: train_loss_1:0.11410252864897986. train_loss_2:-0.1125728420821237\n",
            "Training finished in 252.8031885623932 sec., avg time per epoch: 2.528031885623932 sec.\n",
            "=====================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(train_history[\"TRAIN_LOSSES_1\"])\n",
        "plt.plot(train_history[\"TRAIN_LOSSES_2\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "xZDL8DnpCKh-",
        "outputId": "b6a69ef1-ca59-48b9-e87a-e18e3e0bc23f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### NEW DEEPANT \n",
        "from AnomalyDetectionMethodClass import ADMethod\n",
        "\n",
        "configuration = {\n",
        "    'DATASET': 'NAB', #SWAT, MSL, SMD\n",
        "    'DATAPATH': \"realKnownCause/machine_temperature_system_failure.csv\", #Only needed with NAB\n",
        "    'SEQ_LEN': 30,\n",
        "    'STEP': 30,\n",
        "    'HIDDEN_SIZE': None, #Needed for USAD method\n",
        "    'LR': 0.0001,\n",
        "    'EPOCHS': 15, #50 for SWAT\n",
        "    'VERBOSE': True\n",
        "}\n",
        "method = ADMethod(name = 'DEEPANT', config = configuration)\n",
        "train_losses = method.train()\n",
        "predictions, score = method.test()\n",
        "rep = method.results(0.45, True)"
      ],
      "metadata": {
        "id": "w93QZYTNMKEc",
        "outputId": "2b99451a-7249-46e6-ab6c-24fef037f11c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=====================================================================\n",
            "Initializing...\n",
            "NAB DS\n",
            "FEAT:  1\n",
            "NAB DS\n",
            "FEAT:  1\n",
            "Data preprocessing and method configuration finished in 0.049334049224853516 sec.\n",
            "Model summary: \n",
            "=================================================================\n",
            "Layer (type:depth-idx)                   Param #\n",
            "=================================================================\n",
            "DeepAnt                                  --\n",
            "├─Sequential: 1-1                        --\n",
            "│    └─Conv1d: 2-1                       128\n",
            "│    └─ReLU: 2-2                         --\n",
            "│    └─MaxPool1d: 2-3                    --\n",
            "├─Sequential: 1-2                        --\n",
            "│    └─Conv1d: 2-4                       3,104\n",
            "│    └─ReLU: 2-5                         --\n",
            "│    └─MaxPool1d: 2-6                    --\n",
            "├─Flatten: 1-3                           --\n",
            "├─Sequential: 1-4                        --\n",
            "│    └─Linear: 2-7                       7,720\n",
            "│    └─ReLU: 2-8                         --\n",
            "│    └─Dropout: 2-9                      --\n",
            "├─Linear: 1-5                            41\n",
            "=================================================================\n",
            "Total params: 10,993\n",
            "Trainable params: 10,993\n",
            "Non-trainable params: 0\n",
            "=================================================================\n",
            "=====================================================================\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning:\n",
            "\n",
            "This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15: train_loss:0.7016321023305258\n",
            "Epoch 2/15: train_loss:0.6980091333389282\n",
            "Epoch 3/15: train_loss:0.6897910435994467\n",
            "Epoch 4/15: train_loss:0.6721156438191732\n",
            "Epoch 5/15: train_loss:0.6437764763832092\n",
            "Epoch 6/15: train_loss:0.6070162256558737\n",
            "Epoch 7/15: train_loss:0.5614681839942932\n",
            "Epoch 8/15: train_loss:0.506227026383082\n",
            "Epoch 9/15: train_loss:0.44718440373738605\n",
            "Epoch 10/15: train_loss:0.3781435588995616\n",
            "Epoch 11/15: train_loss:0.2997952699661255\n",
            "Epoch 12/15: train_loss:0.21684221923351288\n",
            "Epoch 13/15: train_loss:0.13366234302520752\n",
            "Epoch 14/15: train_loss:0.0919684146841367\n",
            "Epoch 15/15: train_loss:0.18545797963937125\n",
            "Training finished in 4.454534292221069 sec., avg time per epoch: 0.29696895281473795 sec.\n",
            "=====================================================================\n",
            "=====================================================================\n",
            "Testing...\n",
            "Test finished in 2.1562609672546387 sec.\n",
            "=====================================================================\n",
            "=====================================================================\n",
            "Computing results... \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.95      0.93      0.94       677\n",
            "        True       0.51      0.59      0.55        79\n",
            "\n",
            "    accuracy                           0.90       756\n",
            "   macro avg       0.73      0.76      0.74       756\n",
            "weighted avg       0.91      0.90      0.90       756\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"de1cb356-79a5-421c-9bee-e5c88d0820f4\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"de1cb356-79a5-421c-9bee-e5c88d0820f4\")) {                    Plotly.newPlot(                        \"de1cb356-79a5-421c-9bee-e5c88d0820f4\",                        [{\"line\":{\"color\":\"#515ad6\"},\"mode\":\"lines\",\"name\":\"Anomaly score\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755],\"y\":[0.29946114738871776,0.26039022173480864,0.17705387990909324,0.33159174871903907,0.2833589176822964,0.3988240119535268,0.2927819050617113,0.2789962307077385,0.24294190657974316,0.3406721085509462,0.5425378279398606,0.2795910850667034,0.213622388632667,0.17538433692908992,0.2025821135964556,0.2939695702753816,0.24276597059397373,0.2147741465572062,0.3466649280662173,0.3180905106960419,0.2940522835430277,0.2921082298241662,0.3057334404389057,0.4432112599030892,0.48604573654461913,0.3566055058816431,0.5946214967890222,0.6497976687508787,0.4659754108051747,0.30166531000689434,0.2639936982221026,0.2505114355958007,0.2530155067914891,0.21291630925614988,0.29082734189237247,0.3037693409163559,0.25982874461208205,0.20650457136768333,0.1958987842609302,0.20268711079150273,0.19360062044681706,0.25072571161386725,0.20250183307197564,0.34367742214856956,0.3123262717286538,0.21400968134470363,0.22455211981941262,0.2306426383002326,0.21598419300800423,0.19720166419094506,0.21881629529220406,0.26156183084358287,0.2893277990048134,0.42943132682298824,0.4108097482937956,0.46437836357151996,0.49154821234733925,0.5083219764773197,0.28135102878275753,0.271538802496773,0.3828224978141802,0.4161664541260054,0.4177841310217083,0.469323838498939,0.4622098163132945,0.5414010557132243,0.6064048288978748,0.3199241178751643,0.5502161492303044,0.5921425315028076,0.6445800185180409,0.7090029499443631,0.7050233713635963,0.6757619473233256,0.6634435090276666,0.659819500188051,0.5877566848133088,0.28543424222239844,0.17158390555507172,0.04566901166889589,0.1044341609687767,0.22092820828952367,0.2097136515328958,0.11866016183580638,0.12029759260601772,0.13051706009800065,0.11525665683676248,0.15280293369363881,0.16861965666209147,0.18038771114386123,0.23356300050649711,0.1668716820424144,0.1559117848404096,0.11894586319322839,0.15640553439339308,0.13528805868554997,0.16862637103322983,0.14947669261754895,0.16067538788873412,0.15289469676586256,0.1270444651930406,0.08200008465946221,0.08146089146413674,0.053796320038067585,0.055463235655451554,0.06856258450741573,0.0322378366490812,0.05956065631518234,0.06626510186138909,0.05570028214955239,0.05631148454259319,0.06060430313342188,0.10259092012671675,0.10217852150520632,0.09589338357110153,0.08313831653199813,0.09504883245357235,0.10548792799858706,0.09818025945692413,0.09600927945553264,0.07977577892787066,0.0893922180438507,0.10494143657376345,0.05272357761156196,0.36618117222216007,0.51789725357893,0.557021310343586,0.6499659172682435,0.7021801757705591,0.7187993147449049,0.745773862899353,0.7699400523429019,0.8064406388772793,0.030924155339409037,0.0244074202558954,0.19091243924832127,0.4179646405646299,0.2928950762737964,0.26691756357852625,0.1385104702833124,0.2212528335376027,0.20220425992790322,0.16337115924590803,0.036664942662676336,0.03254776812843718,0.04349375003953207,0.05034036509636339,0.029518029789426636,0.03599885758380922,0.0225005388526113,0.030338448094748532,0.044385593684209496,0.05419616570484112,0.05373958846743507,0.04003808702700781,0.04337902187182072,0.050982998531109686,0.027770444408656114,0.02263200429330514,0.03140681158355524,0.0450027319705755,0.06856433608249532,0.07066369612507803,0.11134285963120585,0.1185378435094169,0.14446251702312032,0.1430297286080379,0.1069870816472396,0.07842940155004666,0.06259623323779143,0.11921278377340846,0.11715536422300862,0.23604323081915815,0.31770321798400514,0.20945081796123474,0.23210170034150848,0.20582486292708632,0.2605283042369142,0.20327126108053692,0.185959666091404,0.21872550531724674,0.19329185668418078,0.2084393806625138,0.2116830057806843,0.25081319305811867,0.21059994852315458,0.20083024658771279,0.22184243317132887,0.24608033988341324,0.15198884052054865,0.11101239579952835,0.12434937500395324,0.13319804037672486,0.13137328838272697,0.2690897112966375,0.23810133153764454,0.2589390417813908,0.45175077227431804,0.29757401985994203,0.257144455802652,0.18080711606568992,0.09875127293286165,0.17285253246130847,0.17957955386409627,0.2513593925537624,0.20621789691299489,0.30466361730419944,0.43552213723298816,0.3250254829846645,0.3448023225885555,0.417026964038704,0.34878686396538106,0.1758848981629383,0.1725486341850043,0.26582312108297945,0.24292332042195444,0.22090534050376273,0.13263208700657278,0.11685652605248983,0.20609090771972657,0.18563796013512426,0.13006038555086794,0.06151667913042083,0.0555730983368308,0.1269071611687482,0.14134870308029077,0.06345294807115096,0.005689894336233348,0.007211429222013621,0.0753096517138919,0.10504769879525691,0.15347232730321178,0.24134914097406066,0.18432710080752476,0.21621870944921237,0.21436369413022863,0.13953699058966287,0.2371647254187116,0.5920945578075729,0.21373526791557215,0.5124663004252921,0.4718773430357618,0.24635942417942355,0.14444928290029693,0.22695430773130645,0.23297632016457023,0.2552259945418974,0.26555152963592055,0.3622129788794104,0.2866082840743388,0.2703270044708954,0.2537606073683898,0.18784718285908628,0.2063749548117957,0.24835738748684497,0.320965526569691,0.31115621957550577,0.2654007968693515,0.23680263592587528,0.2490605475715628,0.2661541687730168,0.2183845320350918,0.18979824287826605,0.20116197544583667,0.27596717353681444,0.21084730984827957,0.16800845426905067,0.26824944180708055,0.18487115948918234,0.166540634352377,0.16917753332493227,0.1456459006088183,0.16879033792262224,0.18011125421047022,0.19355410639748202,0.21304787200657035,0.21672549850556586,0.12749121414805578,0.15015085440372736,0.13876658948383516,0.16852546084670164,0.1803375966346404,0.1983737598483525,0.15107597797491648,0.10152732481451551,0.0827355515734253,0.052363142384078604,0.08400700046173468,0.0920827346757859,0.11215539584866976,0.15409170371329053,0.18291659631984344,0.5213417259728904,0.223170808249724,0.2418997194074032,0.3335490365607239,0.2083831356405145,0.18083134618762384,0.22406430615975434,0.24762483986468112,0.2198621802341564,0.21956480170953727,0.22311641211253094,0.20427559476921295,0.16508964901841247,0.18373662538625882,0.1836398995179763,0.15742592418696508,0.16457400477693448,0.2012698919326831,0.15339710688451721,0.1568146244841976,0.23916045060242017,0.13952443763492603,0.23077838536889875,0.19433287613980102,0.21954670210038177,0.2390771534764143,0.2744862168070441,0.2856383007191675,0.2797710107512651,0.2850522042356005,0.5521079476259588,0.6268087323802494,0.5364851629427044,0.2928023401043063,0.34523574011102065,0.32941901714256794,0.2608947726674493,0.27702473564596386,0.3030496381781089,0.25432276565920287,0.3006462825494953,0.4006474989210783,0.4077301873747441,0.20868440655419931,0.15806680604663179,0.1311414966138648,0.23881539031174628,0.22392495863120246,0.23199699507564128,0.2602929120081662,0.2557778380016864,0.23652316239095836,0.23471154721005713,0.23766081040513445,0.25331054988266877,0.24455140945840811,0.255600247750564,0.25681194846671496,0.23810230463491092,0.2637294050045419,0.23179099038433934,0.23154586718292722,0.1760620991751541,0.12797134033930932,0.08736389410171688,0.10183024999355325,0.1919372079795922,0.20309610357258065,0.161007116746858,0.2341384902298602,0.26795585836180047,0.24988086856715805,0.2605563294381872,0.2525080365670491,0.24950106870407288,0.24350961152497477,0.22341554221222962,0.19466684312163765,0.20748759422622468,0.19702952328451465,0.14562507632731683,0.16425152034284166,0.1429572328616893,0.10187189855655618,0.11909445514581132,0.15227794771840317,0.15160826217965032,0.1363693643680001,0.1698104357870143,0.14654912949151291,0.11213622583252122,0.12270844108358273,0.10717683561419225,0.05912266523556495,0.05574494731408128,0.0945971207024984,0.38122895373068455,0.038289625858697524,0.13645548347607858,0.18174138675118343,0.20604604793574446,0.22194966849008876,0.2556233101557783,0.19744260307411157,0.2726117395427319,0.40364142728068453,0.30145006089156146,0.37031527864883496,0.5236141027094433,0.44021713692402975,0.5805139218593164,0.24827457690947227,0.12688293104681428,0.18879021141997757,0.1999356782706894,0.1770291632385261,0.15382770242490976,0.09537306846274471,0.052711900444364884,0.10383833351254543,0.1582640528625359,0.163278909625051,0.07583424845022099,0.0,0.12203603087248391,0.19543257336058653,0.19109596539276882,0.1922289425400661,0.17209312735459129,0.1371729480906129,0.196630261476101,0.23118883779587632,0.1791823355599421,0.20437543454874804,0.20839919174541052,0.5205304547818729,0.4238952791645765,0.22083936450909922,0.1827187656455795,0.19873302735911622,0.20979996526042757,0.17495296291088422,0.21464414076241198,0.2579813194517765,0.27352547787590387,0.27997925356627984,0.2735218774160181,0.25488871902935495,0.23982527065484097,0.20079570163475477,0.21466856550379923,0.2437048148366194,0.2368694877080786,0.2756163719722686,0.2445520906264946,0.34008426049229956,0.31638467118800095,0.2057075073967556,0.16094036227438133,0.19046510643494627,0.176392465697105,0.19425269292504768,0.20425730054060418,0.19115474046766082,0.20956875734992528,0.20533811967442114,0.20704590537699494,0.20591584752149691,0.18927461923920338,0.17940118513516076,0.18023989766909146,0.18347291602705793,0.20483074675970772,0.21794508130957482,0.2777241979610693,0.39331384868240193,0.5229242740572755,0.20238729952371756,0.19663522427215976,0.2021068528915342,0.2869329093224178,0.2736958672072546,0.20700912230032412,0.22969328460710955,0.23316675529960937,0.25261867772624147,0.2514697417837749,0.2737217515945415,0.2464830075322594,0.2962625766739827,0.29639297170768353,0.3154310358669056,0.34640180256537634,0.41071788791184516,0.396520593414175,0.27526800315088895,0.2946337091597159,0.20426459877010236,0.22321459762671306,0.1715760234672137,0.20590514345156627,0.1964839076472308,0.24685619033393294,0.25333117954471696,0.2770404998216799,0.31492181406738595,0.21213822068191737,0.17354275035238284,0.22457294410091408,0.2332132693489444,0.1878936969084213,0.26702693971127234,0.599098522692385,0.29739516458237336,0.26545314950228505,0.3580634975159259,0.3074124224623934,0.27665379096800313,0.3214041988173949,0.32551125314006324,0.33009619553027225,0.2812429176764578,0.3195374090214874,0.3507149588890732,0.30720680700999803,0.2821574344874429,0.26300473947023617,0.24302549563492895,0.2566463273119697,0.24943849854984182,0.22747540131747637,0.23690734119174248,0.21049368630166113,0.2575712562637055,0.24584348800876568,0.1866971765096266,0.21965393741914171,0.21780894500200212,0.23413294357544154,0.2482095740120752,0.3093688345165385,0.3437133294377006,0.5686212136177177,0.5141965646747203,0.4270320582028937,0.5510813300098818,0.5198273920068817,0.583300093952541,0.49373252378128074,0.5552019103845534,0.5154381394769505,0.5732173466264907,0.6521713449128664,0.6500780180733354,0.4275642450979008,0.5237700902012511,0.3174807679489007,0.44044659325945246,0.32467876842863763,0.48525548425455634,0.4468379934150507,0.6319453236107941,0.6956167320182572,0.5484966863605335,0.42503759804563146,0.244736200629302,0.3164440301212528,0.4612162840042757,0.5016870100759356,0.46786370605067007,0.39181985244926143,0.6073060142763099,0.6570889892584658,0.710412189405598,0.5983924433158678,0.4364686689440386,0.32046700884010204,0.2961004586693965,0.23962919155565657,0.5320673013531404,0.5694328740476419,0.3275881346357915,0.28366515139203996,0.22893884229645114,0.19038716134390574,0.18487991736458012,0.20517775324491447,0.16632986148446963,0.17174232579004553,0.2110377449833187,0.26817772453854516,0.23471271492677684,0.2139413699166007,0.18925661693977455,0.13090046042097161,0.18555115985895929,0.22659367788436977,0.15615038829013675,0.041962581490814194,0.11302505287567274,0.14708588994367225,0.13840518115908537,0.14817974858085928,0.1483726164590645,0.5231489622160927,0.5956432489187673,0.6438230461544898,0.7119954186580696,0.7694652781866137,0.07210319891129877,0.06342044662245236,0.08429854040275522,0.10966689420924416,0.11119436498834959,0.1093879072229604,0.10227077112606328,0.06827221228311489,0.08394754421875619,0.08272046856579576,0.08431177452557861,0.1006844279623392,0.16899663454310407,0.16179046004632916,0.23431452352535623,0.3460567422747024,0.33437393111347136,0.1717850447600415,0.2696461283135786,0.2519662160091043,0.2097003201003458,0.19356481046741267,0.17287501100816285,0.2126360572434198,0.18180502731240752,0.17483794281399292,0.20020999439009426,0.183950998714052,0.1491799952610163,0.13109439870616987,0.08555831212386752,0.05442260543873792,0.08619403656802219,0.10745202752113689,0.16171913201670032,0.15378673502999332,0.2788607755682523,0.5362339092285139,0.5606813237820593,0.6117248489631405,0.6381708106824672,0.68204854490333,0.7454435936871288,0.7898427134233414,0.8403181444202845,0.8650852214258502,0.9206681480450718,0.9568124836702114,1.0,0.9901836867054965,0.9533103066083521,0.9314214566974148,0.9247318022296578,0.9413367339839136,0.9417211074041512,0.9434312285401643,0.9677316080967531,0.9628713764898726,0.18625928273973588,0.1946931167478311,0.16858530632858673,0.1780102399025346,0.19114004669893783,0.10303727984282532,0.07007390187189858,0.08813341942000483,0.08771557145380246,0.1331632034945869,0.1424440213633774,0.1164596969872422,0.2336282953330742,0.14102057468205262,0.17025095691952438,0.11972940111215288,0.15467955177193712,0.14778457378096457,0.14726980532702635,0.1802383407134652,0.16754321646597345,0.1873259919631897,0.19004356069913145,0.07933798246770654,0.0660017817410948,0.07650082007772127,0.08846252091550935,0.10450743519293845,0.08192982703682641,0.05632676216967605,0.07797924675459897,0.12249766821567534,0.08862327658392258,0.20150012674591897,0.15006677879990835,0.15054048254920346,0.16152003631598996,0.14366662076891226,0.07004441702472594,0.0807869242974116,0.12751456848244994,0.13646424135147642,0.44699232664150557,0.11073798237039681,0.11694663485936072,0.09735935460296904,0.06792793047025411,0.07574608583788298,0.07835797621069115,0.09326572902257732,0.062519066624564,0.07261086375519207,0.0793328250521945,0.11263552203992325,0.18571901913741737,0.1780326211396623,0.15821763612292747,0.15711210031854342,0.010013852039587545,0.05201671975723168,0.07663938912846005,0.07431125391854107,0.08152978675059955,0.08462481991618712,0.08493660028034936,0.07948297396040369,0.09629624583940102,0.06284086989057033,0.13830086513212472,0.11568365191726923,0.14462541350551963,0.16636430912770103,0.1389189765157571,0.17530133173226395,0.18858809911774135,0.20045696647631264,0.21615399848099517,0.20049978275603528,0.16074982982961553,0.15191430126994057,0.16268775303569857,0.16720973603277003,0.16731249510410437,0.17378845741215485,0.19314423782886428,0.21721428526249056,0.17970303990720546,0.17101911990163934,0.17513230473708616,0.17845037179613807,0.1703217010907934,0.15188481642276794,0.1182181810573967,0.19277494741625648,0.11258657524742213,0.20524256152285827,0.11374144708321396],\"type\":\"scatter\"},{\"line\":{\"color\":\"green\"},\"mode\":\"markers\",\"name\":\"True positives\",\"x\":[70,71,72,73,74,75,76,125,126,127,128,129,130,131,132,535,537,538,539,540,541,542,543,544,545,547,551,553,554,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658],\"y\":[0.6445800185180409,0.7090029499443631,0.7050233713635963,0.6757619473233256,0.6634435090276666,0.659819500188051,0.5877566848133088,0.51789725357893,0.557021310343586,0.6499659172682435,0.7021801757705591,0.7187993147449049,0.745773862899353,0.7699400523429019,0.8064406388772793,0.5141965646747203,0.5510813300098818,0.5198273920068817,0.583300093952541,0.49373252378128074,0.5552019103845534,0.5154381394769505,0.5732173466264907,0.6521713449128664,0.6500780180733354,0.5237700902012511,0.48525548425455634,0.6319453236107941,0.6956167320182572,0.6381708106824672,0.68204854490333,0.7454435936871288,0.7898427134233414,0.8403181444202845,0.8650852214258502,0.9206681480450718,0.9568124836702114,1.0,0.9901836867054965,0.9533103066083521,0.9314214566974148,0.9247318022296578,0.9413367339839136,0.9417211074041512,0.9434312285401643,0.9677316080967531,0.9628713764898726],\"type\":\"scatter\"},{\"line\":{\"color\":\"red\"},\"mode\":\"markers\",\"name\":\"False negatives\",\"x\":[77,78,79,80,81,82,83,84,85,86,87,88,89,123,124,133,134,135,136,137,138,139,140,141,142,536,546,548,549,550,552,659],\"y\":[0.45,0.45,0.45,0.45,0.45,0.45,0.45,0.45,0.45,0.45,0.45,0.45,0.45,0.45,0.45,0.45,0.45,0.45,0.45,0.45,0.45,0.45,0.45,0.45,0.45,0.45,0.45,0.45,0.45,0.45,0.45,0.45],\"type\":\"scatter\"},{\"line\":{\"color\":\"#000000\"},\"marker\":{\"line\":{\"width\":2},\"size\":10,\"symbol\":34},\"mode\":\"markers\",\"name\":\"False positives\",\"x\":[10,24,26,27,28,55,56,57,63,64,65,66,68,69,197,238,240,241,292,322,323,324,400,402,427,468,504,534,555,559,560,561,563,564,565,566,571,572,596,597,598,599,600,638,639,640],\"y\":[0.5425378279398606,0.48604573654461913,0.5946214967890222,0.6497976687508787,0.4659754108051747,0.46437836357151996,0.49154821234733925,0.5083219764773197,0.469323838498939,0.4622098163132945,0.5414010557132243,0.6064048288978748,0.5502161492303044,0.5921425315028076,0.45175077227431804,0.5920945578075729,0.5124663004252921,0.4718773430357618,0.5213417259728904,0.5521079476259588,0.6268087323802494,0.5364851629427044,0.5236141027094433,0.5805139218593164,0.5205304547818729,0.5229242740572755,0.599098522692385,0.5686212136177177,0.5484966863605335,0.4612162840042757,0.5016870100759356,0.46786370605067007,0.6073060142763099,0.6570889892584658,0.710412189405598,0.5983924433158678,0.5320673013531404,0.5694328740476419,0.5231489622160927,0.5956432489187673,0.6438230461544898,0.7119954186580696,0.7694652781866137,0.5362339092285139,0.5606813237820593,0.6117248489631405],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"shapes\":[{\"line\":{\"color\":\"#aaaaaa\",\"dash\":\"dash\"},\"name\":\"threshold\",\"type\":\"line\",\"x0\":0,\"x1\":1,\"xref\":\"x domain\",\"y0\":0.45,\"y1\":0.45,\"yref\":\"y\"}],\"plot_bgcolor\":\"white\",\"xaxis\":{\"showgrid\":false,\"gridwidth\":1,\"gridcolor\":\"LightGrey\",\"showline\":true,\"linewidth\":2,\"linecolor\":\"DarkGrey\"},\"yaxis\":{\"showgrid\":true,\"gridwidth\":1,\"gridcolor\":\"LightGrey\",\"showline\":true,\"linewidth\":2,\"linecolor\":\"DarkGrey\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('de1cb356-79a5-421c-9bee-e5c88d0820f4');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "method.train_ds.n_features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHw6ZaxZxI4u",
        "outputId": "6b15a33f-b94b-4858-a4f3-1de99f16635d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "51"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fzB2-pxe5AmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir input\n",
        "!mkdir input/SWAT\n",
        "#normal period\n",
        "!python USAD/gdrivedl.py https://drive.google.com/open?id=1rVJ5ry5GG-ZZi5yI4x9lICB8VhErXwCw input\n",
        "#anomalies\n",
        "!python USAD/gdrivedl.py https://drive.google.com/open?id=1iDYc0OEmidN712fquOBRFjln90SbpaE7 input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2mlFBfe5Aj1",
        "outputId": "a6ca5f40-5412-4f9d-adbb-558dfe447571"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input/SWaT_Dataset_Normal_v1.csv\n",
            "[==================================================] 163.77MB/163.77MB\n",
            "input/SWaT_Dataset_Attack_v0.csv\n",
            "[==================================================] 127.27MB/127.27MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### USAD\n",
        "from AnomalyDetectionMethodClass import ADMethod_back\n",
        "settings_={\n",
        "    'EPOCHS':20,\n",
        "    'BATCH_SIZE': 256,\n",
        "    'SEQ_LEN': 12,\n",
        "    'HIDDEN_SIZE': 100,\n",
        "    'ALPHA': 0.5,\n",
        "    'BETA': 0.5,\n",
        "    'CONFIDENCE': 0.82,\n",
        "    'DATASET': 'SWAT'}\n",
        "data_path = ''\n",
        "method_ = ADMethod_back('USAD',settings_,'')\n",
        "\n",
        "method_.prepare_pipeline()\n",
        "#h = method.train()\n",
        "#scores, report = method.test()"
      ],
      "metadata": {
        "id": "8HysJsSBIfX0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "890de6a9-5377-4002-e1d7-93f3b2da93d2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/Time-Series-Anomaly-Detection-An-experimental-survey/USAD/USADSolver.py:22: DtypeWarning: Columns (26) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  self.prepare_data()\n",
            "/content/Time-Series-Anomaly-Detection-An-experimental-survey/USAD/USADSolver.py:22: DtypeWarning: Columns (1,9,28,46) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  self.prepare_data()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(494988, 12, 51)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for [el] in method_.solver.train_loader:\n",
        "  print(el.size())\n",
        "  break\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUupyW1h0kl2",
        "outputId": "a73ed80e-c9df-4bf7-f64b-0cd63ade522b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([256, 612])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for el in method.train_dl:\n",
        "  print(el.size())\n",
        "  break\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvapzMfnBtCl",
        "outputId": "dc93836b-e16d-40ff-d303-6b0db73d0bfe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([256, 612])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for el in method.solver.train_loader:\n",
        "  print(el[0].shape)\n",
        "  print(len(el))\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyC6JV0Z0rJy",
        "outputId": "8630b255-8b0e-4984-d8af-8835be170c13"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([7919, 612])\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###  TRANSFORMER\n",
        "from AnomalyDetectionMethodClass import ADMethod\n",
        "data_path = './dataset/MSL'\n",
        "settings = {\n",
        "    'lr':1e-4,\n",
        "    'num_epochs':3,\n",
        "    'k':3,\n",
        "    'win_size':100,\n",
        "    'input_c':55,\n",
        "    'output_c':55,\n",
        "    'batch_size':256,\n",
        "    'pretrained_model':None,\n",
        "    'dataset':'MSL',\n",
        "    'mode':'train', # choices=['train', 'test']\n",
        "    'data_path':'./dataset/MSL',\n",
        "    'model_save_path':'checkpoints',\n",
        "    'anormly_ratio':1\n",
        "}\n",
        "method = ADMethod('transformer',settings, data_path)\n",
        "\n",
        "method.prepare_pipeline()\n",
        "method.train()\n",
        "scores, report = method.test()"
      ],
      "metadata": {
        "id": "UEinh9TJbwD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### DEEPANT\n",
        "from AnomalyDetectionMethodClass import ADMethod\n",
        "# data_path = './NAB/realKnownCause/cpu_utilization_asg_misconfiguration.csv'\n",
        "data_path = './dataset/MSL'\n",
        "\n",
        "\n",
        "LR_LIST = [0.001, 0.0001, 0.00001, 0.000001]\n",
        "SEQ_LIST = [100, 30, 100, 300]\n",
        "for lr in [0.0001]:#LR_LIST:\n",
        "  for seq in [100]: #SEQ_LIST:\n",
        "    # wandb.init(project=\"experimental-survey-AD\", entity=\"michiamoantonio\", group='DeepAnt')\n",
        "    # wandb.log({'lr': lr})\n",
        "    # wandb.log({'seq_len': seq})\n",
        "    settings = {\n",
        "      'SEQ_LEN': seq,\n",
        "      'out_dim': 1,\n",
        "      'EPOCHS': 1,\n",
        "      'LR': lr,\n",
        "      'CONFIDENCE': 0.50,\n",
        "      'TH_LIST': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
        "      'TH_SEARCH': True,\n",
        "      'VERBOSE': True,\n",
        "      'DATASET': 'SWAT'\n",
        "    }\n",
        "    # wandb.config = settings\n",
        "    method = ADMethod('DeepAnt',settings, data_path)\n",
        "\n",
        "    method.prepare_pipeline()\n",
        "    l = method.train()\n",
        "    scores, report = method.test()\n",
        "\n",
        "    print(f\"lr: {lr}, seq_len: {seq}\")\n",
        "    for i,el in enumerate(report): \n",
        "      print(f\"#### th: {settings['TH_LIST'][i]}, f1-score: {el['macro avg']['f1-score']}\")\n",
        "      # wandb.log({'th': settings['TH_LIST'][i], 'f1-score': el['macro avg']['f1-score']})\n",
        "    # wandb.finish()\n",
        "\n"
      ],
      "metadata": {
        "id": "a8y-8dwBgXf1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "162a387d-a240-4cc9-c0bd-14a825c53010"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOAD TRAIN DATA\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/Time-Series-Anomaly-Detection-An-experimental-survey/DeepAnt/DeepAntSolver.py:22: DtypeWarning: Columns (26) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  self.prepare_data(data_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SCALER\n",
            "SCALED\n",
            "window\n",
            "WINDOWED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo\n",
        "from torchinfo import summary\n",
        "summary(method.solver.model)"
      ],
      "metadata": {
        "id": "z1jw6nzI8RZZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}