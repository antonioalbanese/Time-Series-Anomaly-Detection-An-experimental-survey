{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNj1tEC/aT6dEoH6nZ+Tt3R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antonioalbanese/Time-Series-Anomaly-Detection-An-experimental-survey/blob/main/TadGAN/TadGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbQGvmWWN7V7",
        "outputId": "f3b09640-60df-4940-b304-520bbfe9884c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'TadGAN'...\n",
            "remote: Enumerating objects: 55, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 55 (delta 9), reused 8 (delta 8), pack-reused 42\u001b[K\n",
            "Unpacking objects: 100% (55/55), done.\n",
            "/content/TadGAN\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/arunppsg/TadGAN\n",
        "%cd TadGAN"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#to have nab dataset\n",
        "!git clone https://github.com/antonioalbanese/Time-Series-Anomaly-Detection-An-experimental-survey/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQhJZTICQJan",
        "outputId": "82fdd5b0-b831-4079-bbbf-e962855d6608"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Time-Series-Anomaly-Detection-An-experimental-survey'...\n",
            "remote: Enumerating objects: 114, done.\u001b[K\n",
            "remote: Counting objects: 100% (97/97), done.\u001b[K\n",
            "remote: Compressing objects: 100% (79/79), done.\u001b[K\n",
            "remote: Total 114 (delta 13), reused 92 (delta 12), pack-reused 17\u001b[K\n",
            "Receiving objects: 100% (114/114), 1.90 MiB | 21.16 MiB/s, done.\n",
            "Resolving deltas: 100% (13/13), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##################################\n",
        "########### FUNCTIONS ############\n",
        "##################################\n",
        "import os\n",
        "import logging\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import model\n",
        "import anomaly_detection\n",
        "\n",
        "# logging.basicConfig(filename='train.log', level=logging.DEBUG)\n",
        "\n",
        "class SignalDataset(Dataset):\n",
        "    def __init__(self, path):\n",
        "        self.signal_df = pd.read_csv(path)\n",
        "        self.signal_columns = self.make_signal_list()\n",
        "        self.make_rolling_signals()\n",
        "\n",
        "    def make_signal_list(self):\n",
        "        signal_list = list()\n",
        "        for i in range(-50, 50):\n",
        "            signal_list.append('signal'+str(i))\n",
        "        return signal_list\n",
        "\n",
        "    def make_rolling_signals(self):\n",
        "        for i in range(-50, 50):\n",
        "            self.signal_df['signal'+str(i)] = self.signal_df['signal'].shift(i)\n",
        "        self.signal_df = self.signal_df.dropna()\n",
        "        self.signal_df = self.signal_df.reset_index(drop=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.signal_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.signal_df.loc[idx]\n",
        "        x = row[self.signal_columns].values.astype(float)\n",
        "        x = torch.from_numpy(x)\n",
        "        return {'signal':x, 'anomaly':row['anomaly']}\n",
        "\n",
        "def critic_x_iteration(sample):\n",
        "    optim_cx.zero_grad()\n",
        "\n",
        "    x = sample['signal'].view(1, batch_size, signal_shape)\n",
        "    valid_x = critic_x(x)\n",
        "    valid_x = torch.squeeze(valid_x)\n",
        "    critic_score_valid_x = torch.mean(torch.ones(valid_x.shape) * valid_x) #Wasserstein Loss\n",
        "\n",
        "    #The sampled z are the anomalous points - points deviating from actual distribution of z (obtained through encoding x)\n",
        "    z = torch.empty(1, batch_size, latent_space_dim).uniform_(0, 1)\n",
        "    x_ = decoder(z)\n",
        "    fake_x = critic_x(x_)\n",
        "    fake_x = torch.squeeze(fake_x)\n",
        "    critic_score_fake_x = torch.mean(torch.ones(fake_x.shape) * fake_x)  #Wasserstein Loss\n",
        "\n",
        "    alpha = torch.rand(x.shape)\n",
        "    ix = Variable(alpha * x + (1 - alpha) * x_) #Random Weighted Average\n",
        "    ix.requires_grad_(True)\n",
        "    v_ix = critic_x(ix)\n",
        "    v_ix.mean().backward()\n",
        "    gradients = ix.grad\n",
        "    #Gradient Penalty Loss\n",
        "    gp_loss = torch.sqrt(torch.sum(torch.square(gradients).view(-1)))\n",
        "\n",
        "    #Critic has to maximize Cx(Valid X) - Cx(Fake X).\n",
        "    #Maximizing the above is same as minimizing the negative.\n",
        "    wl = critic_score_fake_x - critic_score_valid_x\n",
        "    loss = wl + gp_loss\n",
        "    loss.backward()\n",
        "    optim_cx.step()\n",
        "\n",
        "    return loss\n",
        "\n",
        "def critic_z_iteration(sample):\n",
        "    optim_cz.zero_grad()\n",
        "\n",
        "    x = sample['signal'].view(1, batch_size, signal_shape)\n",
        "    z = encoder(x)\n",
        "    valid_z = critic_z(z)\n",
        "    valid_z = torch.squeeze(valid_z)\n",
        "    critic_score_valid_z = torch.mean(torch.ones(valid_z.shape) * valid_z)\n",
        "\n",
        "    z_ = torch.empty(1, batch_size, latent_space_dim).uniform_(0, 1)\n",
        "    fake_z = critic_z(z_)\n",
        "    fake_z = torch.squeeze(fake_z)\n",
        "    critic_score_fake_z = torch.mean(torch.ones(fake_z.shape) * fake_z) #Wasserstein Loss\n",
        "\n",
        "    wl = critic_score_fake_z - critic_score_valid_z\n",
        "\n",
        "    alpha = torch.rand(z.shape)\n",
        "    iz = Variable(alpha * z + (1 - alpha) * z_) #Random Weighted Average\n",
        "    iz.requires_grad_(True)\n",
        "    v_iz = critic_z(iz)\n",
        "    v_iz.mean().backward()\n",
        "    gradients = iz.grad\n",
        "    gp_loss = torch.sqrt(torch.sum(torch.square(gradients).view(-1)))\n",
        "\n",
        "    loss = wl + gp_loss\n",
        "    loss.backward()\n",
        "    optim_cz.step()\n",
        "\n",
        "    return loss\n",
        "\n",
        "def encoder_iteration(sample):\n",
        "    optim_enc.zero_grad()\n",
        "\n",
        "    x = sample['signal'].view(1, batch_size, signal_shape)\n",
        "    valid_x = critic_x(x)\n",
        "    valid_x = torch.squeeze(valid_x)\n",
        "    critic_score_valid_x = torch.mean(torch.ones(valid_x.shape) * valid_x) #Wasserstein Loss\n",
        "\n",
        "    z = torch.empty(1, batch_size, latent_space_dim).uniform_(0, 1)\n",
        "    x_ = decoder(z)\n",
        "    fake_x = critic_x(x_)\n",
        "    fake_x = torch.squeeze(fake_x)\n",
        "    critic_score_fake_x = torch.mean(torch.ones(fake_x.shape) * fake_x)\n",
        "\n",
        "    enc_z = encoder(x)\n",
        "    gen_x = decoder(enc_z)\n",
        "\n",
        "    mse = mse_loss(x.float(), gen_x.float())\n",
        "    loss_enc = mse + critic_score_valid_x - critic_score_fake_x\n",
        "    loss_enc.backward(retain_graph=True)\n",
        "    optim_enc.step()\n",
        "\n",
        "    return loss_enc\n",
        "\n",
        "def decoder_iteration(sample):\n",
        "    optim_dec.zero_grad()\n",
        "\n",
        "    x = sample['signal'].view(1, batch_size, signal_shape)\n",
        "    z = encoder(x)\n",
        "    valid_z = critic_z(z)\n",
        "    valid_z = torch.squeeze(valid_z)\n",
        "    critic_score_valid_z = torch.mean(torch.ones(valid_z.shape) * valid_z)\n",
        "\n",
        "    z_ = torch.empty(1, batch_size, latent_space_dim).uniform_(0, 1)\n",
        "    fake_z = critic_z(z_)\n",
        "    fake_z = torch.squeeze(fake_z)\n",
        "    critic_score_fake_z = torch.mean(torch.ones(fake_z.shape) * fake_z)\n",
        "\n",
        "    enc_z = encoder(x)\n",
        "    gen_x = decoder(enc_z)\n",
        "\n",
        "    mse = mse_loss(x.float(), gen_x.float())\n",
        "    loss_dec = mse + critic_score_valid_z - critic_score_fake_z\n",
        "    loss_dec.backward(retain_graph=True)\n",
        "    optim_dec.step()\n",
        "\n",
        "    return loss_dec\n",
        "\n",
        "\n",
        "def train(n_epochs=2000):\n",
        "    # logging.debug('Starting training')\n",
        "    print('Starting training')\n",
        "    cx_epoch_loss = list()\n",
        "    cz_epoch_loss = list()\n",
        "    encoder_epoch_loss = list()\n",
        "    decoder_epoch_loss = list()\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        print('Epoch {}'.format(epoch))\n",
        "        n_critics = 5\n",
        "\n",
        "        cx_nc_loss = list()\n",
        "        cz_nc_loss = list()\n",
        "\n",
        "        for i in range(n_critics):\n",
        "            cx_loss = list()\n",
        "            cz_loss = list()\n",
        "\n",
        "            for batch, sample in enumerate(train_loader):\n",
        "                loss = critic_x_iteration(sample)\n",
        "                cx_loss.append(loss)\n",
        "\n",
        "                loss = critic_z_iteration(sample)\n",
        "                cz_loss.append(loss)\n",
        "\n",
        "            cx_nc_loss.append(torch.mean(torch.tensor(cx_loss)))\n",
        "            cz_nc_loss.append(torch.mean(torch.tensor(cz_loss)))\n",
        "\n",
        "        print('Critic training done in epoch {}'.format(epoch))\n",
        "        encoder_loss = list()\n",
        "        decoder_loss = list()\n",
        "\n",
        "        for batch, sample in enumerate(train_loader):\n",
        "            enc_loss = encoder_iteration(sample)\n",
        "            dec_loss = decoder_iteration(sample)\n",
        "            encoder_loss.append(enc_loss)\n",
        "            decoder_loss.append(dec_loss)\n",
        "\n",
        "        cx_epoch_loss.append(torch.mean(torch.tensor(cx_nc_loss)))\n",
        "        cz_epoch_loss.append(torch.mean(torch.tensor(cz_nc_loss)))\n",
        "        encoder_epoch_loss.append(torch.mean(torch.tensor(encoder_loss)))\n",
        "        decoder_epoch_loss.append(torch.mean(torch.tensor(decoder_loss)))\n",
        "        print('Encoder decoder training done in epoch {}'.format(epoch))\n",
        "        print('critic x loss {:.3f} critic z loss {:.3f} \\nencoder loss {:.3f} decoder loss {:.3f}\\n'.format(cx_epoch_loss[-1], cz_epoch_loss[-1], encoder_epoch_loss[-1], decoder_epoch_loss[-1]))\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            torch.save(encoder.state_dict(), encoder.encoder_path)\n",
        "            torch.save(decoder.state_dict(), decoder.decoder_path)\n",
        "            torch.save(critic_x.state_dict(), critic_x.critic_x_path)\n",
        "            torch.save(critic_z.state_dict(), critic_z.critic_z_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "Pp21is8UO2cc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##################################\n",
        "############## MAIN ##############\n",
        "##################################\n",
        "\n",
        "# The dataset should have a column name as signal containing the signals and a column with name anomaly containing the true labels (used during validation)\n",
        "\n",
        "\n",
        "import json\n",
        "from sklearn import preprocessing\n",
        "base_folder = \"./Time-Series-Anomaly-Detection-An-experimental-survey/NAB/\"\n",
        "file_name = \"artificialWithAnomaly/art_daily_flatmiddle.csv\"\n",
        "label_file = \"combined_windows.json\"\n",
        "with open(base_folder + label_file) as FI:\n",
        "    j_label = json.load(FI)\n",
        "\n",
        "dataset = pd.read_csv(base_folder + file_name)\n",
        "\n",
        "dataset['timestamp'] = pd.to_datetime(dataset['timestamp'])\n",
        "\n",
        "\n",
        "anomalies_span = j_label[file_name]\n",
        "y = np.zeros(len(dataset))\n",
        "for w in anomalies_span:\n",
        "  start = pd.to_datetime(w[0])\n",
        "  end = pd.to_datetime(w[1])\n",
        "  for idx in dataset.index:\n",
        "      if dataset.loc[idx, 'timestamp'] >= start and dataset.loc[idx, 'timestamp'] <= end:\n",
        "          y[idx] = 1.0\n",
        "dataset['anomaly'] = y\n",
        "dataset = dataset.reset_index().drop(columns=['timestamp', 'index'])\n",
        "dataset.columns = ['signal', 'anomaly']\n",
        "scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
        "dataset['signal'] = scaler.fit_transform(np.array(dataset['signal']).reshape(-1,1))\n",
        "\n",
        "\n",
        "#Splitting intro train and test\n",
        "#TODO could be done in a more pythonic way\n",
        "train_len = int(0.7 * dataset.shape[0])\n",
        "dataset[0:train_len].to_csv('train_dataset.csv', index=False)\n",
        "dataset[train_len:].to_csv('test_dataset.csv', index=False)\n",
        "\n",
        "train_dataset = SignalDataset(path='train_dataset.csv')\n",
        "test_dataset = SignalDataset(path='test_dataset.csv')\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, drop_last=True)\n",
        "\n",
        "logging.info('Number of train datapoints is {}'.format(len(train_dataset)))\n",
        "logging.info('Number of samples in train dataset {}'.format(len(train_dataset)))\n",
        "\n",
        "lr = 1e-6\n",
        "\n",
        "signal_shape = 100\n",
        "latent_space_dim = 20\n",
        "encoder_path = 'models/encoder.pt'\n",
        "decoder_path = 'models/decoder.pt'\n",
        "critic_x_path = 'models/critic_x.pt'\n",
        "critic_z_path = 'models/critic_z.pt'\n",
        "\n",
        "encoder = model.Encoder(encoder_path, signal_shape)\n",
        "decoder = model.Decoder(decoder_path, signal_shape)\n",
        "critic_x = model.CriticX(critic_x_path, signal_shape)\n",
        "critic_z = model.CriticZ(critic_z_path)\n",
        "\n",
        "mse_loss = torch.nn.MSELoss()\n",
        "\n",
        "optim_enc = optim.Adam(encoder.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "optim_dec = optim.Adam(decoder.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "optim_cx = optim.Adam(critic_x.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "optim_cz = optim.Adam(critic_z.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "train(n_epochs=2000)\n",
        "\n",
        "anomaly_detection.test(test_loader, encoder, decoder, critic_x)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TegqO-w5QVmN",
        "outputId": "32092a1e-d2ef-46fc-a101-23a7e898b8d5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training\n",
            "Epoch 0\n",
            "Critic training done in epoch 0\n",
            "Encoder decoder training done in epoch 0\n",
            "critic x loss 0.055 critic z loss -0.005 \n",
            "encoder loss 0.307 decoder loss 0.394\n",
            "\n",
            "Epoch 1\n",
            "Critic training done in epoch 1\n",
            "Encoder decoder training done in epoch 1\n",
            "critic x loss 0.043 critic z loss -0.008 \n",
            "encoder loss 0.319 decoder loss 0.396\n",
            "\n",
            "Epoch 2\n",
            "Critic training done in epoch 2\n",
            "Encoder decoder training done in epoch 2\n",
            "critic x loss 0.030 critic z loss -0.012 \n",
            "encoder loss 0.331 decoder loss 0.401\n",
            "\n",
            "Epoch 3\n",
            "Critic training done in epoch 3\n",
            "Encoder decoder training done in epoch 3\n",
            "critic x loss 0.018 critic z loss -0.012 \n",
            "encoder loss 0.343 decoder loss 0.404\n",
            "\n",
            "Epoch 4\n",
            "Critic training done in epoch 4\n",
            "Encoder decoder training done in epoch 4\n",
            "critic x loss 0.006 critic z loss -0.015 \n",
            "encoder loss 0.355 decoder loss 0.406\n",
            "\n",
            "Epoch 5\n",
            "Critic training done in epoch 5\n",
            "Encoder decoder training done in epoch 5\n",
            "critic x loss -0.006 critic z loss -0.018 \n",
            "encoder loss 0.367 decoder loss 0.403\n",
            "\n",
            "Epoch 6\n",
            "Critic training done in epoch 6\n",
            "Encoder decoder training done in epoch 6\n",
            "critic x loss -0.018 critic z loss -0.018 \n",
            "encoder loss 0.379 decoder loss 0.406\n",
            "\n",
            "Epoch 7\n",
            "Critic training done in epoch 7\n",
            "Encoder decoder training done in epoch 7\n",
            "critic x loss -0.031 critic z loss -0.024 \n",
            "encoder loss 0.392 decoder loss 0.414\n",
            "\n",
            "Epoch 8\n",
            "Critic training done in epoch 8\n",
            "Encoder decoder training done in epoch 8\n",
            "critic x loss -0.043 critic z loss -0.024 \n",
            "encoder loss 0.404 decoder loss 0.413\n",
            "\n",
            "Epoch 9\n",
            "Critic training done in epoch 9\n",
            "Encoder decoder training done in epoch 9\n",
            "critic x loss -0.055 critic z loss -0.030 \n",
            "encoder loss 0.416 decoder loss 0.420\n",
            "\n",
            "Epoch 10\n",
            "Critic training done in epoch 10\n",
            "Encoder decoder training done in epoch 10\n",
            "critic x loss -0.067 critic z loss -0.031 \n",
            "encoder loss 0.428 decoder loss 0.419\n",
            "\n",
            "Epoch 11\n",
            "Critic training done in epoch 11\n",
            "Encoder decoder training done in epoch 11\n",
            "critic x loss -0.079 critic z loss -0.032 \n",
            "encoder loss 0.440 decoder loss 0.416\n",
            "\n",
            "Epoch 12\n",
            "Critic training done in epoch 12\n",
            "Encoder decoder training done in epoch 12\n",
            "critic x loss -0.091 critic z loss -0.030 \n",
            "encoder loss 0.452 decoder loss 0.429\n",
            "\n",
            "Epoch 13\n",
            "Critic training done in epoch 13\n",
            "Encoder decoder training done in epoch 13\n",
            "critic x loss -0.103 critic z loss -0.037 \n",
            "encoder loss 0.464 decoder loss 0.431\n",
            "\n",
            "Epoch 14\n",
            "Critic training done in epoch 14\n",
            "Encoder decoder training done in epoch 14\n",
            "critic x loss -0.115 critic z loss -0.039 \n",
            "encoder loss 0.476 decoder loss 0.431\n",
            "\n",
            "Epoch 15\n",
            "Critic training done in epoch 15\n",
            "Encoder decoder training done in epoch 15\n",
            "critic x loss -0.127 critic z loss -0.041 \n",
            "encoder loss 0.488 decoder loss 0.438\n",
            "\n",
            "Epoch 16\n",
            "Critic training done in epoch 16\n",
            "Encoder decoder training done in epoch 16\n",
            "critic x loss -0.139 critic z loss -0.042 \n",
            "encoder loss 0.500 decoder loss 0.435\n",
            "\n",
            "Epoch 17\n",
            "Critic training done in epoch 17\n",
            "Encoder decoder training done in epoch 17\n",
            "critic x loss -0.151 critic z loss -0.049 \n",
            "encoder loss 0.512 decoder loss 0.437\n",
            "\n",
            "Epoch 18\n",
            "Critic training done in epoch 18\n",
            "Encoder decoder training done in epoch 18\n",
            "critic x loss -0.163 critic z loss -0.051 \n",
            "encoder loss 0.524 decoder loss 0.434\n",
            "\n",
            "Epoch 19\n",
            "Critic training done in epoch 19\n",
            "Encoder decoder training done in epoch 19\n",
            "critic x loss -0.175 critic z loss -0.052 \n",
            "encoder loss 0.536 decoder loss 0.440\n",
            "\n",
            "Epoch 20\n",
            "Critic training done in epoch 20\n",
            "Encoder decoder training done in epoch 20\n",
            "critic x loss -0.187 critic z loss -0.057 \n",
            "encoder loss 0.547 decoder loss 0.449\n",
            "\n",
            "Epoch 21\n",
            "Critic training done in epoch 21\n",
            "Encoder decoder training done in epoch 21\n",
            "critic x loss -0.199 critic z loss -0.058 \n",
            "encoder loss 0.559 decoder loss 0.449\n",
            "\n",
            "Epoch 22\n",
            "Critic training done in epoch 22\n",
            "Encoder decoder training done in epoch 22\n",
            "critic x loss -0.211 critic z loss -0.063 \n",
            "encoder loss 0.571 decoder loss 0.453\n",
            "\n",
            "Epoch 23\n",
            "Critic training done in epoch 23\n",
            "Encoder decoder training done in epoch 23\n",
            "critic x loss -0.223 critic z loss -0.065 \n",
            "encoder loss 0.583 decoder loss 0.458\n",
            "\n",
            "Epoch 24\n",
            "Critic training done in epoch 24\n",
            "Encoder decoder training done in epoch 24\n",
            "critic x loss -0.235 critic z loss -0.065 \n",
            "encoder loss 0.595 decoder loss 0.451\n",
            "\n",
            "Epoch 25\n",
            "Critic training done in epoch 25\n",
            "Encoder decoder training done in epoch 25\n",
            "critic x loss -0.247 critic z loss -0.063 \n",
            "encoder loss 0.607 decoder loss 0.457\n",
            "\n",
            "Epoch 26\n",
            "Critic training done in epoch 26\n",
            "Encoder decoder training done in epoch 26\n",
            "critic x loss -0.258 critic z loss -0.068 \n",
            "encoder loss 0.619 decoder loss 0.455\n",
            "\n",
            "Epoch 27\n",
            "Critic training done in epoch 27\n",
            "Encoder decoder training done in epoch 27\n",
            "critic x loss -0.270 critic z loss -0.071 \n",
            "encoder loss 0.631 decoder loss 0.460\n",
            "\n",
            "Epoch 28\n",
            "Critic training done in epoch 28\n",
            "Encoder decoder training done in epoch 28\n",
            "critic x loss -0.282 critic z loss -0.071 \n",
            "encoder loss 0.642 decoder loss 0.460\n",
            "\n",
            "Epoch 29\n",
            "Critic training done in epoch 29\n",
            "Encoder decoder training done in epoch 29\n",
            "critic x loss -0.294 critic z loss -0.075 \n",
            "encoder loss 0.654 decoder loss 0.468\n",
            "\n",
            "Epoch 30\n",
            "Critic training done in epoch 30\n",
            "Encoder decoder training done in epoch 30\n",
            "critic x loss -0.306 critic z loss -0.077 \n",
            "encoder loss 0.666 decoder loss 0.466\n",
            "\n",
            "Epoch 31\n",
            "Critic training done in epoch 31\n",
            "Encoder decoder training done in epoch 31\n",
            "critic x loss -0.317 critic z loss -0.081 \n",
            "encoder loss 0.678 decoder loss 0.470\n",
            "\n",
            "Epoch 32\n",
            "Critic training done in epoch 32\n",
            "Encoder decoder training done in epoch 32\n",
            "critic x loss -0.329 critic z loss -0.087 \n",
            "encoder loss 0.690 decoder loss 0.477\n",
            "\n",
            "Epoch 33\n",
            "Critic training done in epoch 33\n",
            "Encoder decoder training done in epoch 33\n",
            "critic x loss -0.341 critic z loss -0.088 \n",
            "encoder loss 0.701 decoder loss 0.476\n",
            "\n",
            "Epoch 34\n",
            "Critic training done in epoch 34\n",
            "Encoder decoder training done in epoch 34\n",
            "critic x loss -0.353 critic z loss -0.086 \n",
            "encoder loss 0.713 decoder loss 0.473\n",
            "\n",
            "Epoch 35\n",
            "Critic training done in epoch 35\n",
            "Encoder decoder training done in epoch 35\n",
            "critic x loss -0.364 critic z loss -0.091 \n",
            "encoder loss 0.725 decoder loss 0.480\n",
            "\n",
            "Epoch 36\n",
            "Critic training done in epoch 36\n",
            "Encoder decoder training done in epoch 36\n",
            "critic x loss -0.376 critic z loss -0.096 \n",
            "encoder loss 0.737 decoder loss 0.477\n",
            "\n",
            "Epoch 37\n",
            "Critic training done in epoch 37\n",
            "Encoder decoder training done in epoch 37\n",
            "critic x loss -0.388 critic z loss -0.098 \n",
            "encoder loss 0.749 decoder loss 0.483\n",
            "\n",
            "Epoch 38\n",
            "Critic training done in epoch 38\n",
            "Encoder decoder training done in epoch 38\n",
            "critic x loss -0.400 critic z loss -0.099 \n",
            "encoder loss 0.760 decoder loss 0.490\n",
            "\n",
            "Epoch 39\n",
            "Critic training done in epoch 39\n",
            "Encoder decoder training done in epoch 39\n",
            "critic x loss -0.411 critic z loss -0.103 \n",
            "encoder loss 0.772 decoder loss 0.493\n",
            "\n",
            "Epoch 40\n",
            "Critic training done in epoch 40\n",
            "Encoder decoder training done in epoch 40\n",
            "critic x loss -0.423 critic z loss -0.105 \n",
            "encoder loss 0.784 decoder loss 0.494\n",
            "\n",
            "Epoch 41\n",
            "Critic training done in epoch 41\n",
            "Encoder decoder training done in epoch 41\n",
            "critic x loss -0.435 critic z loss -0.107 \n",
            "encoder loss 0.796 decoder loss 0.486\n",
            "\n",
            "Epoch 42\n",
            "Critic training done in epoch 42\n",
            "Encoder decoder training done in epoch 42\n",
            "critic x loss -0.446 critic z loss -0.111 \n",
            "encoder loss 0.807 decoder loss 0.500\n",
            "\n",
            "Epoch 43\n",
            "Critic training done in epoch 43\n",
            "Encoder decoder training done in epoch 43\n",
            "critic x loss -0.458 critic z loss -0.112 \n",
            "encoder loss 0.819 decoder loss 0.498\n",
            "\n",
            "Epoch 44\n",
            "Critic training done in epoch 44\n",
            "Encoder decoder training done in epoch 44\n",
            "critic x loss -0.470 critic z loss -0.115 \n",
            "encoder loss 0.831 decoder loss 0.496\n",
            "\n",
            "Epoch 45\n",
            "Critic training done in epoch 45\n",
            "Encoder decoder training done in epoch 45\n",
            "critic x loss -0.481 critic z loss -0.116 \n",
            "encoder loss 0.842 decoder loss 0.504\n",
            "\n",
            "Epoch 46\n",
            "Critic training done in epoch 46\n",
            "Encoder decoder training done in epoch 46\n",
            "critic x loss -0.493 critic z loss -0.117 \n",
            "encoder loss 0.854 decoder loss 0.507\n",
            "\n",
            "Epoch 47\n",
            "Critic training done in epoch 47\n",
            "Encoder decoder training done in epoch 47\n",
            "critic x loss -0.505 critic z loss -0.121 \n",
            "encoder loss 0.866 decoder loss 0.504\n",
            "\n",
            "Epoch 48\n",
            "Critic training done in epoch 48\n",
            "Encoder decoder training done in epoch 48\n",
            "critic x loss -0.516 critic z loss -0.124 \n",
            "encoder loss 0.878 decoder loss 0.514\n",
            "\n",
            "Epoch 49\n",
            "Critic training done in epoch 49\n",
            "Encoder decoder training done in epoch 49\n",
            "critic x loss -0.528 critic z loss -0.124 \n",
            "encoder loss 0.889 decoder loss 0.506\n",
            "\n",
            "Epoch 50\n",
            "Critic training done in epoch 50\n",
            "Encoder decoder training done in epoch 50\n",
            "critic x loss -0.539 critic z loss -0.130 \n",
            "encoder loss 0.901 decoder loss 0.516\n",
            "\n",
            "Epoch 51\n",
            "Critic training done in epoch 51\n",
            "Encoder decoder training done in epoch 51\n",
            "critic x loss -0.551 critic z loss -0.131 \n",
            "encoder loss 0.913 decoder loss 0.511\n",
            "\n",
            "Epoch 52\n",
            "Critic training done in epoch 52\n",
            "Encoder decoder training done in epoch 52\n",
            "critic x loss -0.563 critic z loss -0.133 \n",
            "encoder loss 0.924 decoder loss 0.516\n",
            "\n",
            "Epoch 53\n",
            "Critic training done in epoch 53\n",
            "Encoder decoder training done in epoch 53\n",
            "critic x loss -0.574 critic z loss -0.134 \n",
            "encoder loss 0.936 decoder loss 0.519\n",
            "\n",
            "Epoch 54\n",
            "Critic training done in epoch 54\n",
            "Encoder decoder training done in epoch 54\n",
            "critic x loss -0.586 critic z loss -0.134 \n",
            "encoder loss 0.948 decoder loss 0.529\n",
            "\n",
            "Epoch 55\n",
            "Critic training done in epoch 55\n",
            "Encoder decoder training done in epoch 55\n",
            "critic x loss -0.597 critic z loss -0.137 \n",
            "encoder loss 0.959 decoder loss 0.527\n",
            "\n",
            "Epoch 56\n",
            "Critic training done in epoch 56\n",
            "Encoder decoder training done in epoch 56\n",
            "critic x loss -0.609 critic z loss -0.144 \n",
            "encoder loss 0.971 decoder loss 0.530\n",
            "\n",
            "Epoch 57\n",
            "Critic training done in epoch 57\n",
            "Encoder decoder training done in epoch 57\n",
            "critic x loss -0.620 critic z loss -0.148 \n",
            "encoder loss 0.983 decoder loss 0.533\n",
            "\n",
            "Epoch 58\n",
            "Critic training done in epoch 58\n",
            "Encoder decoder training done in epoch 58\n",
            "critic x loss -0.632 critic z loss -0.148 \n",
            "encoder loss 0.995 decoder loss 0.536\n",
            "\n",
            "Epoch 59\n",
            "Critic training done in epoch 59\n",
            "Encoder decoder training done in epoch 59\n",
            "critic x loss -0.644 critic z loss -0.149 \n",
            "encoder loss 1.006 decoder loss 0.541\n",
            "\n",
            "Epoch 60\n",
            "Critic training done in epoch 60\n",
            "Encoder decoder training done in epoch 60\n",
            "critic x loss -0.655 critic z loss -0.155 \n",
            "encoder loss 1.018 decoder loss 0.538\n",
            "\n",
            "Epoch 61\n",
            "Critic training done in epoch 61\n",
            "Encoder decoder training done in epoch 61\n",
            "critic x loss -0.667 critic z loss -0.157 \n",
            "encoder loss 1.030 decoder loss 0.536\n",
            "\n",
            "Epoch 62\n",
            "Critic training done in epoch 62\n",
            "Encoder decoder training done in epoch 62\n",
            "critic x loss -0.678 critic z loss -0.158 \n",
            "encoder loss 1.041 decoder loss 0.540\n",
            "\n",
            "Epoch 63\n",
            "Critic training done in epoch 63\n",
            "Encoder decoder training done in epoch 63\n",
            "critic x loss -0.690 critic z loss -0.157 \n",
            "encoder loss 1.053 decoder loss 0.542\n",
            "\n",
            "Epoch 64\n",
            "Critic training done in epoch 64\n",
            "Encoder decoder training done in epoch 64\n",
            "critic x loss -0.701 critic z loss -0.163 \n",
            "encoder loss 1.065 decoder loss 0.546\n",
            "\n",
            "Epoch 65\n",
            "Critic training done in epoch 65\n",
            "Encoder decoder training done in epoch 65\n",
            "critic x loss -0.713 critic z loss -0.166 \n",
            "encoder loss 1.077 decoder loss 0.546\n",
            "\n",
            "Epoch 66\n",
            "Critic training done in epoch 66\n",
            "Encoder decoder training done in epoch 66\n",
            "critic x loss -0.724 critic z loss -0.169 \n",
            "encoder loss 1.088 decoder loss 0.550\n",
            "\n",
            "Epoch 67\n",
            "Critic training done in epoch 67\n",
            "Encoder decoder training done in epoch 67\n",
            "critic x loss -0.736 critic z loss -0.167 \n",
            "encoder loss 1.100 decoder loss 0.550\n",
            "\n",
            "Epoch 68\n",
            "Critic training done in epoch 68\n",
            "Encoder decoder training done in epoch 68\n",
            "critic x loss -0.748 critic z loss -0.170 \n",
            "encoder loss 1.112 decoder loss 0.553\n",
            "\n",
            "Epoch 69\n",
            "Critic training done in epoch 69\n",
            "Encoder decoder training done in epoch 69\n",
            "critic x loss -0.759 critic z loss -0.175 \n",
            "encoder loss 1.123 decoder loss 0.557\n",
            "\n",
            "Epoch 70\n",
            "Critic training done in epoch 70\n",
            "Encoder decoder training done in epoch 70\n",
            "critic x loss -0.771 critic z loss -0.179 \n",
            "encoder loss 1.135 decoder loss 0.563\n",
            "\n",
            "Epoch 71\n",
            "Critic training done in epoch 71\n",
            "Encoder decoder training done in epoch 71\n",
            "critic x loss -0.782 critic z loss -0.179 \n",
            "encoder loss 1.147 decoder loss 0.567\n",
            "\n",
            "Epoch 72\n",
            "Critic training done in epoch 72\n",
            "Encoder decoder training done in epoch 72\n",
            "critic x loss -0.794 critic z loss -0.183 \n",
            "encoder loss 1.159 decoder loss 0.567\n",
            "\n",
            "Epoch 73\n",
            "Critic training done in epoch 73\n",
            "Encoder decoder training done in epoch 73\n",
            "critic x loss -0.805 critic z loss -0.185 \n",
            "encoder loss 1.170 decoder loss 0.563\n",
            "\n",
            "Epoch 74\n",
            "Critic training done in epoch 74\n",
            "Encoder decoder training done in epoch 74\n",
            "critic x loss -0.817 critic z loss -0.185 \n",
            "encoder loss 1.182 decoder loss 0.575\n",
            "\n",
            "Epoch 75\n",
            "Critic training done in epoch 75\n",
            "Encoder decoder training done in epoch 75\n",
            "critic x loss -0.829 critic z loss -0.191 \n",
            "encoder loss 1.194 decoder loss 0.572\n",
            "\n",
            "Epoch 76\n",
            "Critic training done in epoch 76\n",
            "Encoder decoder training done in epoch 76\n",
            "critic x loss -0.840 critic z loss -0.189 \n",
            "encoder loss 1.206 decoder loss 0.582\n",
            "\n",
            "Epoch 77\n",
            "Critic training done in epoch 77\n",
            "Encoder decoder training done in epoch 77\n",
            "critic x loss -0.852 critic z loss -0.197 \n",
            "encoder loss 1.217 decoder loss 0.577\n",
            "\n",
            "Epoch 78\n",
            "Critic training done in epoch 78\n",
            "Encoder decoder training done in epoch 78\n",
            "critic x loss -0.863 critic z loss -0.199 \n",
            "encoder loss 1.229 decoder loss 0.583\n",
            "\n",
            "Epoch 79\n",
            "Critic training done in epoch 79\n",
            "Encoder decoder training done in epoch 79\n",
            "critic x loss -0.875 critic z loss -0.195 \n",
            "encoder loss 1.241 decoder loss 0.581\n",
            "\n",
            "Epoch 80\n",
            "Critic training done in epoch 80\n",
            "Encoder decoder training done in epoch 80\n",
            "critic x loss -0.887 critic z loss -0.204 \n",
            "encoder loss 1.253 decoder loss 0.588\n",
            "\n",
            "Epoch 81\n",
            "Critic training done in epoch 81\n",
            "Encoder decoder training done in epoch 81\n",
            "critic x loss -0.898 critic z loss -0.202 \n",
            "encoder loss 1.265 decoder loss 0.588\n",
            "\n",
            "Epoch 82\n",
            "Critic training done in epoch 82\n",
            "Encoder decoder training done in epoch 82\n",
            "critic x loss -0.910 critic z loss -0.204 \n",
            "encoder loss 1.277 decoder loss 0.591\n",
            "\n",
            "Epoch 83\n",
            "Critic training done in epoch 83\n",
            "Encoder decoder training done in epoch 83\n",
            "critic x loss -0.922 critic z loss -0.210 \n",
            "encoder loss 1.288 decoder loss 0.593\n",
            "\n",
            "Epoch 84\n",
            "Critic training done in epoch 84\n",
            "Encoder decoder training done in epoch 84\n",
            "critic x loss -0.933 critic z loss -0.212 \n",
            "encoder loss 1.300 decoder loss 0.597\n",
            "\n",
            "Epoch 85\n",
            "Critic training done in epoch 85\n",
            "Encoder decoder training done in epoch 85\n",
            "critic x loss -0.945 critic z loss -0.213 \n",
            "encoder loss 1.312 decoder loss 0.595\n",
            "\n",
            "Epoch 86\n",
            "Critic training done in epoch 86\n",
            "Encoder decoder training done in epoch 86\n",
            "critic x loss -0.956 critic z loss -0.217 \n",
            "encoder loss 1.324 decoder loss 0.606\n",
            "\n",
            "Epoch 87\n",
            "Critic training done in epoch 87\n",
            "Encoder decoder training done in epoch 87\n",
            "critic x loss -0.968 critic z loss -0.219 \n",
            "encoder loss 1.336 decoder loss 0.606\n",
            "\n",
            "Epoch 88\n",
            "Critic training done in epoch 88\n",
            "Encoder decoder training done in epoch 88\n",
            "critic x loss -0.980 critic z loss -0.220 \n",
            "encoder loss 1.348 decoder loss 0.604\n",
            "\n",
            "Epoch 89\n",
            "Critic training done in epoch 89\n",
            "Encoder decoder training done in epoch 89\n",
            "critic x loss -0.991 critic z loss -0.223 \n",
            "encoder loss 1.360 decoder loss 0.608\n",
            "\n",
            "Epoch 90\n",
            "Critic training done in epoch 90\n",
            "Encoder decoder training done in epoch 90\n",
            "critic x loss -1.003 critic z loss -0.226 \n",
            "encoder loss 1.371 decoder loss 0.613\n",
            "\n",
            "Epoch 91\n",
            "Critic training done in epoch 91\n",
            "Encoder decoder training done in epoch 91\n",
            "critic x loss -1.015 critic z loss -0.229 \n",
            "encoder loss 1.383 decoder loss 0.614\n",
            "\n",
            "Epoch 92\n",
            "Critic training done in epoch 92\n",
            "Encoder decoder training done in epoch 92\n",
            "critic x loss -1.027 critic z loss -0.231 \n",
            "encoder loss 1.395 decoder loss 0.620\n",
            "\n",
            "Epoch 93\n",
            "Critic training done in epoch 93\n",
            "Encoder decoder training done in epoch 93\n",
            "critic x loss -1.038 critic z loss -0.235 \n",
            "encoder loss 1.407 decoder loss 0.616\n",
            "\n",
            "Epoch 94\n",
            "Critic training done in epoch 94\n",
            "Encoder decoder training done in epoch 94\n",
            "critic x loss -1.050 critic z loss -0.239 \n",
            "encoder loss 1.419 decoder loss 0.623\n",
            "\n",
            "Epoch 95\n",
            "Critic training done in epoch 95\n",
            "Encoder decoder training done in epoch 95\n",
            "critic x loss -1.062 critic z loss -0.238 \n",
            "encoder loss 1.431 decoder loss 0.627\n",
            "\n",
            "Epoch 96\n",
            "Critic training done in epoch 96\n",
            "Encoder decoder training done in epoch 96\n",
            "critic x loss -1.073 critic z loss -0.238 \n",
            "encoder loss 1.443 decoder loss 0.627\n",
            "\n",
            "Epoch 97\n",
            "Critic training done in epoch 97\n",
            "Encoder decoder training done in epoch 97\n",
            "critic x loss -1.085 critic z loss -0.244 \n",
            "encoder loss 1.455 decoder loss 0.631\n",
            "\n",
            "Epoch 98\n",
            "Critic training done in epoch 98\n",
            "Encoder decoder training done in epoch 98\n",
            "critic x loss -1.097 critic z loss -0.247 \n",
            "encoder loss 1.467 decoder loss 0.626\n",
            "\n",
            "Epoch 99\n",
            "Critic training done in epoch 99\n",
            "Encoder decoder training done in epoch 99\n",
            "critic x loss -1.109 critic z loss -0.250 \n",
            "encoder loss 1.479 decoder loss 0.636\n",
            "\n",
            "0 877 0 211\n",
            "Accuracy 0.81\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ZeroDivisionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-93217ceb55c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m \u001b[0manomaly_detection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/TadGAN/anomaly_detection.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(test_loader, encoder, decoder, critic_x)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0my_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manomaly_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0my_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprune_false_positive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_predict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manomaly_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchange_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mfind_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m#Other error metrics - point wise difference, Area difference.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/TadGAN/anomaly_detection.py\u001b[0m in \u001b[0;36mfind_scores\u001b[0;34m(y_true, y_predict)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy {:.2f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Precision {:.2f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
          ]
        }
      ]
    }
  ]
}